{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d80feb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'inference'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-38e12d779cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutilities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mget_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint16_to_float32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnote_to_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTargetProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRegressionPostProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_midi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0minference\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPianoTranscription\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'inference'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../pytorch'))\n",
    "import librosa\n",
    "import mir_eval\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utilities import (get_filename, traverse_folder, int16_to_float32, note_to_freq, TargetProcessor, RegressionPostProcessor, read_midi)\n",
    "import config\n",
    "from inference import PianoTranscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2e8878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "def plot(args):\n",
    "    workspace = args.workspace\n",
    "    probs_dir = os.path.join(workspace, 'probs', 'model_type=Note_pedal', \n",
    "        'augmentation=random_target_none', 'dataset=maestro', 'split=test')\n",
    "    prob_names = os.listdir(probs_dir)\n",
    "    for prob_name in prob_names:\n",
    "        prob_path = os.path.join(probs_dir, prob_name)\n",
    "        total_dict = pickle.load(open(prob_path, 'rb'))\n",
    "        import crash\n",
    "        asdf\n",
    "'''\n",
    "\n",
    "def plot(args):\n",
    "    \"\"\"Inference the output probabilites of MAESTRO dataset.\n",
    "    Args:\n",
    "      cuda: bool\n",
    "      audio_path: str\n",
    "    \"\"\"\n",
    "\n",
    "    # Arugments & parameters\n",
    "    workspace = args.workspace\n",
    "    model_type = args.model_type\n",
    "    checkpoint_path = args.checkpoint_path\n",
    "    dataset = args.dataset\n",
    "    split = args.split\n",
    "    post_processor_type = args.post_processor_type\n",
    "    device = torch.device('cuda') if args.cuda and torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    sample_rate = config.sample_rate\n",
    "    segment_seconds = config.segment_seconds\n",
    "    segment_samples = int(segment_seconds * sample_rate)\n",
    "    frames_per_second = config.frames_per_second\n",
    "    classes_num = config.classes_num\n",
    "    begin_note = config.begin_note\n",
    "\n",
    "    # Paths\n",
    "    hdf5s_dir = os.path.join(workspace, 'hdf5s', dataset)\n",
    "\n",
    "    # Transcriptor\n",
    "    transcriptor = PianoTranscription(model_type, device=device, \n",
    "        checkpoint_path=checkpoint_path, segment_samples=segment_samples, \n",
    "        post_processor_type=post_processor_type)\n",
    "\n",
    "    (hdf5_names, hdf5_paths) = traverse_folder(hdf5s_dir)\n",
    "\n",
    "    n = 0\n",
    "    for n, hdf5_path in enumerate(hdf5_paths):\n",
    "        with h5py.File(hdf5_path, 'r') as hf:\n",
    "            if hf.attrs['split'].decode() == split:\n",
    "                print(n, hdf5_path)\n",
    "                \n",
    "                if n == 90:\n",
    "                    # Load audio                \n",
    "                    audio = int16_to_float32(hf['waveform'][:])\n",
    "                    midi_events = [e.decode() for e in hf['midi_event'][:]]\n",
    "                    midi_events_time = hf['midi_event_time'][:]\n",
    "            \n",
    "                    # Ground truths processor\n",
    "                    target_processor = TargetProcessor(\n",
    "                        segment_seconds=len(audio) / sample_rate, \n",
    "                        frames_per_second=frames_per_second, begin_note=begin_note, \n",
    "                        classes_num=classes_num)\n",
    "\n",
    "                    # Get ground truths\n",
    "                    (target_dict, note_events, pedal_events) = \\\n",
    "                        target_processor.process(start_time=0, \n",
    "                            midi_events_time=midi_events_time, \n",
    "                            midi_events=midi_events, extend_pedal=True)\n",
    "\n",
    "                    ref_on_off_pairs = np.array([[event['onset_time'], event['offset_time']] for event in note_events])\n",
    "                    ref_midi_notes = np.array([event['midi_note'] for event in note_events])\n",
    "                    ref_velocity = np.array([event['velocity'] for event in note_events])\n",
    "\n",
    "                    # Transcribe\n",
    "                    transcribed_dict = transcriptor.transcribe(audio, midi_path=None)\n",
    "                    output_dict = transcribed_dict['output_dict']\n",
    "\n",
    "                    # Pack probabilites to dump\n",
    "                    total_dict = {key: output_dict[key] for key in output_dict.keys()}\n",
    "                    total_dict['frame_roll'] = target_dict['frame_roll']\n",
    "                    total_dict['ref_on_off_pairs'] = ref_on_off_pairs\n",
    "                    total_dict['ref_midi_notes'] = ref_midi_notes\n",
    "                    total_dict['ref_velocity'] = ref_velocity\n",
    "\n",
    "                    if 'pedal_frame_output' in output_dict.keys():\n",
    "                        total_dict['ref_pedal_on_off_pairs'] = \\\n",
    "                            np.array([[event['onset_time'], event['offset_time']] for event in pedal_events])\n",
    "                        total_dict['pedal_frame_roll'] = target_dict['pedal_frame_roll']\n",
    "\n",
    "                    if True:\n",
    "                        post_processor = RegressionPostProcessor(100, \n",
    "                        classes_num=100, onset_threshold=0.3, \n",
    "                        offset_threshold=0.3, \n",
    "                        frame_threshold=0.3, \n",
    "                        pedal_offset_threshold=0.2)\n",
    "\n",
    "                        (est_on_off_note_vels, est_pedal_on_offs) = \\\n",
    "                            post_processor.output_dict_to_note_pedal_arrays(output_dict)\n",
    "\n",
    "                        ref_on_off_pairs = total_dict['ref_on_off_pairs']\n",
    "                        ref_midi_notes = total_dict['ref_midi_notes']\n",
    "\n",
    "                        est_on_offs = est_on_off_note_vels[:, 0 : 2]\n",
    "                        est_midi_notes = est_on_off_note_vels[:, 2]\n",
    "                        est_vels = est_on_off_note_vels[:, 3] * 128\n",
    "\n",
    "                        note_precision, note_recall, note_f1, _ = \\\n",
    "                        mir_eval.transcription.precision_recall_f1_overlap(\n",
    "                            ref_intervals=ref_on_off_pairs, \n",
    "                            ref_pitches=note_to_freq(ref_midi_notes), \n",
    "                            est_intervals=est_on_offs, \n",
    "                            est_pitches=note_to_freq(est_midi_notes), \n",
    "                            onset_tolerance=0.05, \n",
    "                            offset_ratio=0.2, \n",
    "                            offset_min_tolerance=0.05)\n",
    "\n",
    "                        print('note f1: {:.3f}'.format(note_f1))\n",
    "                        \n",
    "                    if True:\n",
    "                        librosa.output.write_wav('_zz.wav', audio, sr=16000)\n",
    "                        bgn = 15500\n",
    "                        L = 500\n",
    "                        fontsize = 7\n",
    "                        vmin = 0\n",
    "                        vmax = 1\n",
    "\n",
    "                        \n",
    "                        fig, axs = plt.subplots(7, 1, figsize=(4, 6), sharex=True)\n",
    "                        mel = librosa.feature.melspectrogram(audio, sr=16000, n_fft=2048, hop_length=160, n_mels=229, fmin=30, fmax=8000).T\n",
    "                        axs[0].matshow(np.log(mel[bgn : bgn + L]).T, origin='lower', aspect='auto', cmap='jet')\n",
    "                        axs[1].matshow(target_dict['frame_roll'][bgn : bgn + L].T, origin='lower', aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "                        axs[2].matshow(output_dict['frame_output'][bgn : bgn + L].T, origin='lower', aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "                        axs[3].matshow(target_dict['reg_onset_roll'][bgn : bgn + L].T, origin='lower', aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "                        axs[4].matshow(output_dict['reg_onset_output'][bgn : bgn + L].T, origin='lower', aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "                        axs[5].matshow(target_dict['reg_offset_roll'][bgn : bgn + L].T, origin='lower', aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "                        axs[6].matshow(output_dict['reg_offset_output'][bgn : bgn + L].T, origin='lower', aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "                        axs[0].set_xlim(0, len(output_dict['frame_output'][bgn : bgn + L]))\n",
    "                        axs[0].set_title('Log mel spectrogram', fontsize=fontsize)\n",
    "                        axs[1].set_title('Frame-wise target', fontsize=fontsize)\n",
    "                        axs[2].set_title('Frame-wise output', fontsize=fontsize)\n",
    "                        axs[3].set_title('Regression onsets target', fontsize=fontsize)\n",
    "                        axs[4].set_title('Regression onsets output', fontsize=fontsize)\n",
    "                        axs[5].set_title('Regression offsets target', fontsize=fontsize)\n",
    "                        axs[6].set_title('Regression offsets output', fontsize=fontsize)\n",
    "                        axs[0].set_ylabel('Mel bins', fontsize=fontsize)\n",
    "                        axs[0].yaxis.set_ticks(np.arange(0, 229, 228))\n",
    "                        axs[0].yaxis.set_ticklabels([0, 228], fontsize=fontsize)\n",
    "                        for i in range(6):\n",
    "                            axs[i].xaxis.set_ticks([])\n",
    "                            axs[i].xaxis.set_ticklabels([])\n",
    "                        for i in range(1, 7):\n",
    "                            axs[i].yaxis.set_ticks(np.arange(0, 88, 87))\n",
    "                            axs[i].yaxis.set_ticklabels([0, 87], fontsize=fontsize)\n",
    "                            axs[i].set_ylabel('Note', fontsize=fontsize)\n",
    "                        axs[6].xaxis.set_ticks([0, 100, 200, 300, 400, 499])\n",
    "                        axs[6].xaxis.set_ticklabels(['0', '1', '2', '3', '4', '5'], fontsize=fontsize)\n",
    "                        axs[6].set_xlabel('Seconds', fontsize=fontsize)\n",
    "                        axs[6].xaxis.set_ticks_position('bottom')\n",
    "                        plt.tight_layout(0, 0, 0)\n",
    "                        fig_path = '_zz.pdf'\n",
    "                        plt.savefig(fig_path)\n",
    "                        print('Plot to {}'.format(fig_path))\n",
    "\n",
    "                        fig, axs = plt.subplots(7, 1, figsize=(4, 6), sharex=True)\n",
    "                        mel = librosa.feature.melspectrogram(audio, sr=16000, n_fft=2048, hop_length=160, n_mels=229, fmin=30, fmax=8000).T\n",
    "                        axs[0].matshow(np.log(mel[bgn : bgn + L]).T, origin='lower', aspect='auto', cmap='jet')\n",
    "                        axs[1].plot(target_dict['pedal_frame_roll'][bgn : bgn + L])\n",
    "                        axs[2].plot(output_dict['pedal_frame_output'][bgn : bgn + L])\n",
    "                        axs[3].plot(target_dict['reg_pedal_onset_roll'][bgn : bgn + L])\n",
    "                        axs[4].plot(output_dict['reg_pedal_onset_output'][bgn : bgn + L])\n",
    "                        axs[5].plot(target_dict['reg_pedal_offset_roll'][bgn : bgn + L])\n",
    "                        axs[6].plot(output_dict['reg_pedal_offset_output'][bgn : bgn + L])\n",
    "                        axs[0].set_xlim(0, len(output_dict['frame_output'][bgn : bgn + L]))\n",
    "                        axs[0].set_title('Log mel spectrogram', fontsize=fontsize)\n",
    "                        axs[1].set_title('Frame-wise target', fontsize=fontsize)\n",
    "                        axs[2].set_title('Frame-wise output', fontsize=fontsize)\n",
    "                        axs[3].set_title('Regression onsets target', fontsize=fontsize)\n",
    "                        axs[4].set_title('Regression onsets output', fontsize=fontsize)\n",
    "                        axs[5].set_title('Regression offsets target', fontsize=fontsize)\n",
    "                        axs[6].set_title('Regression offsets output', fontsize=fontsize)\n",
    "                        axs[0].set_ylabel('Mel bins', fontsize=fontsize)\n",
    "                        axs[0].yaxis.set_ticks(np.arange(0, 229, 228))\n",
    "                        axs[0].yaxis.set_ticklabels([0, 228], fontsize=fontsize)\n",
    "                        for i in range(6):\n",
    "                            axs[i].xaxis.set_ticks([])\n",
    "                            axs[i].xaxis.set_ticklabels([])\n",
    "                        for i in range(1, 7):\n",
    "                            axs[i].set_ylim(0, 1.02)\n",
    "                            axs[i].set_ylabel('Value', fontsize=fontsize)\n",
    "                        axs[6].xaxis.set_ticks([0, 100, 200, 300, 400, 499])\n",
    "                        axs[6].xaxis.set_ticklabels(['0', '1', '2', '3', '4', '5'], fontsize=fontsize)\n",
    "                        axs[6].set_xlabel('Seconds', fontsize=fontsize)\n",
    "                        axs[6].xaxis.set_ticks_position('bottom')\n",
    "                        plt.tight_layout(0, 0, 0)\n",
    "                        fig_path = '_zz2.pdf'\n",
    "                        plt.savefig(fig_path)\n",
    "                        print('Plot to {}'.format(fig_path))\n",
    "\n",
    "\n",
    "                        import crash\n",
    "                        asdf\n",
    "\n",
    "                n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b06be447",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def plot2(args):\n",
    "#     # Onsets frames plot\n",
    "#     \"\"\"Inference the output probabilites of MAESTRO dataset.\n",
    "\n",
    "#     Args:\n",
    "#       cuda: bool\n",
    "#       audio_path: str\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Arugments & parameters\n",
    "#     workspace = args.workspace\n",
    "#     model_type = args.model_type\n",
    "#     checkpoint_path = args.checkpoint_path\n",
    "#     dataset = args.dataset\n",
    "#     split = args.split\n",
    "#     post_processor_type = args.post_processor_type\n",
    "#     device = torch.device('cuda') if args.cuda and torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "#     sample_rate = config.sample_rate\n",
    "#     segment_seconds = config.segment_seconds\n",
    "#     segment_samples = int(segment_seconds * sample_rate)\n",
    "#     frames_per_second = config.frames_per_second\n",
    "#     classes_num = config.classes_num\n",
    "#     begin_note = config.begin_note\n",
    "\n",
    "#     # Paths\n",
    "#     hdf5s_dir = os.path.join(workspace, 'hdf5s', dataset)\n",
    "\n",
    "#     # Transcriptor\n",
    "#     transcriptor = PianoTranscription(model_type, device=device, \n",
    "#         checkpoint_path=checkpoint_path, segment_samples=segment_samples, \n",
    "#         post_processor_type=post_processor_type)\n",
    "\n",
    "#     (hdf5_names, hdf5_paths) = traverse_folder(hdf5s_dir)\n",
    "\n",
    "#     n = 0\n",
    "#     for n, hdf5_path in enumerate(hdf5_paths):\n",
    "#         with h5py.File(hdf5_path, 'r') as hf:\n",
    "#             if hf.attrs['split'].decode() == split:\n",
    "#                 print(n, hdf5_path)\n",
    "                \n",
    "#                 if n == 90:\n",
    "#                     # Load audio                \n",
    "#                     audio = int16_to_float32(hf['waveform'][:])\n",
    "#                     midi_events = [e.decode() for e in hf['midi_event'][:]]\n",
    "#                     midi_events_time = hf['midi_event_time'][:]\n",
    "            \n",
    "#                     # Ground truths processor\n",
    "#                     target_processor = TargetProcessor(\n",
    "#                         segment_seconds=len(audio) / sample_rate, \n",
    "#                         frames_per_second=frames_per_second, begin_note=begin_note, \n",
    "#                         classes_num=classes_num)\n",
    "\n",
    "#                     # Get ground truths\n",
    "#                     (target_dict, note_events, pedal_events) = \\\n",
    "#                         target_processor.process(start_time=0, \n",
    "#                             midi_events_time=midi_events_time, \n",
    "#                             midi_events=midi_events, extend_pedal=True)\n",
    "\n",
    "#                     ref_on_off_pairs = np.array([[event['onset_time'], event['offset_time']] for event in note_events])\n",
    "#                     ref_midi_notes = np.array([event['midi_note'] for event in note_events])\n",
    "#                     ref_velocity = np.array([event['velocity'] for event in note_events])\n",
    "\n",
    "#                     # Transcribe\n",
    "#                     transcribed_dict = transcriptor.transcribe(audio, midi_path=None)\n",
    "#                     output_dict = transcribed_dict['output_dict']\n",
    "\n",
    "#                     # Pack probabilites to dump\n",
    "#                     total_dict = {key: output_dict[key] for key in output_dict.keys()}\n",
    "#                     total_dict['frame_roll'] = target_dict['frame_roll']\n",
    "#                     total_dict['ref_on_off_pairs'] = ref_on_off_pairs\n",
    "#                     total_dict['ref_midi_notes'] = ref_midi_notes\n",
    "#                     total_dict['ref_velocity'] = ref_velocity\n",
    "\n",
    "#                     if 'pedal_frame_output' in output_dict.keys():\n",
    "#                         total_dict['ref_pedal_on_off_pairs'] = \\\n",
    "#                             np.array([[event['onset_time'], event['offset_time']] for event in pedal_events])\n",
    "#                         total_dict['pedal_frame_roll'] = target_dict['pedal_frame_roll']\n",
    "\n",
    "#                     if True:\n",
    "#                         post_processor = RegressionPostProcessor(100, \n",
    "#                         classes_num=100, onset_threshold=0.3, \n",
    "#                         offset_threshold=0.3, \n",
    "#                         frame_threshold=0.3, \n",
    "#                         pedal_offset_threshold=0.2)\n",
    "\n",
    "#                         (est_on_off_note_vels, est_pedal_on_offs) = \\\n",
    "#                             post_processor.output_dict_to_note_pedal_arrays(output_dict)\n",
    "\n",
    "#                         ref_on_off_pairs = total_dict['ref_on_off_pairs']\n",
    "#                         ref_midi_notes = total_dict['ref_midi_notes']\n",
    "\n",
    "#                         est_on_offs = est_on_off_note_vels[:, 0 : 2]\n",
    "#                         est_midi_notes = est_on_off_note_vels[:, 2]\n",
    "#                         est_vels = est_on_off_note_vels[:, 3] * 128\n",
    "\n",
    "#                         note_precision, note_recall, note_f1, _ = \\\n",
    "#                         mir_eval.transcription.precision_recall_f1_overlap(\n",
    "#                             ref_intervals=ref_on_off_pairs, \n",
    "#                             ref_pitches=note_to_freq(ref_midi_notes), \n",
    "#                             est_intervals=est_on_offs, \n",
    "#                             est_pitches=note_to_freq(est_midi_notes), \n",
    "#                             onset_tolerance=0.05, \n",
    "#                             offset_ratio=0.2, \n",
    "#                             offset_min_tolerance=0.05)\n",
    "\n",
    "#                         print('note f1: {:.3f}'.format(note_f1))\n",
    "                        \n",
    "#                     if True:\n",
    "#                         librosa.output.write_wav('_zz.wav', audio, sr=16000)\n",
    "#                         bgn = 8000\n",
    "#                         L = 1000\n",
    "\n",
    "#                         import matplotlib.pyplot as plt\n",
    "#                         fig, axs = plt.subplots(7, 1, figsize=(8, 8), sharex=True)\n",
    "#                         mel = librosa.feature.melspectrogram(audio, sr=16000, n_fft=2048, hop_length=160, n_mels=229, fmin=30, fmax=8000).T\n",
    "#                         axs[0].matshow(np.log(mel[bgn : bgn + L]).T, origin='lower', aspect='auto', cmap='jet')\n",
    "#                         axs[1].matshow(target_dict['frame_roll'][bgn : bgn + L].T, origin='lower', aspect='auto', cmap='jet')\n",
    "#                         axs[2].matshow(output_dict['frame_output'][bgn : bgn + L].T, origin='lower', aspect='auto', cmap='jet')\n",
    "#                         axs[3].matshow(target_dict['onset_roll'][bgn : bgn + L].T, origin='lower', aspect='auto', cmap='jet')\n",
    "#                         axs[4].matshow(output_dict['reg_onset_output'][bgn : bgn + L].T, origin='lower', aspect='auto', cmap='jet')\n",
    "#                         axs[5].matshow(target_dict['offset_roll'][bgn : bgn + L].T, origin='lower', aspect='auto', cmap='jet')\n",
    "#                         axs[6].matshow(output_dict['reg_offset_output'][bgn : bgn + L].T, origin='lower', aspect='auto', cmap='jet')\n",
    "#                         axs[0].set_xlim(0, len(output_dict['frame_output'][bgn : bgn + L]))\n",
    "#                         axs[6].set_xlabel('Frames')\n",
    "#                         axs[0].set_title('Log mel spectrogram')\n",
    "#                         axs[1].set_title('Ground truth frames')\n",
    "#                         axs[2].set_title('frame_output')\n",
    "#                         axs[3].set_title('Ground truth reg onset')\n",
    "#                         axs[4].set_title('reg_onset_output')\n",
    "#                         axs[5].set_title('Ground truth reg offset')\n",
    "#                         axs[6].set_title('reg_offset_output')\n",
    "#                         plt.tight_layout(0, .05, 0)\n",
    "#                         fig_path = '_zz.pdf'\n",
    "#                         plt.savefig(fig_path)\n",
    "#                         print('Plot to {}'.format(fig_path))\n",
    "\n",
    "#                         fig, axs = plt.subplots(7, 1, figsize=(8, 8), sharex=True)\n",
    "#                         mel = librosa.feature.melspectrogram(audio, sr=16000, n_fft=2048, hop_length=160, n_mels=229, fmin=30, fmax=8000).T\n",
    "#                         axs[0].matshow(np.log(mel[bgn : bgn + L]).T, origin='lower', aspect='auto', cmap='jet')\n",
    "#                         axs[1].plot(target_dict['pedal_frame_roll'][bgn : bgn + L])\n",
    "#                         axs[2].plot(output_dict['pedal_frame_output'][bgn : bgn + L])\n",
    "#                         axs[3].plot(target_dict['pedal_onset_roll'][bgn : bgn + L])\n",
    "#                         axs[4].plot(output_dict['reg_pedal_onset_output'][bgn : bgn + L])\n",
    "#                         axs[5].plot(target_dict['pedal_offset_roll'][bgn : bgn + L])\n",
    "#                         axs[6].plot(output_dict['reg_pedal_offset_output'][bgn : bgn + L])\n",
    "#                         axs[0].set_xlim(0, len(output_dict['frame_output'][bgn : bgn + L]))\n",
    "#                         axs[6].set_xlabel('Frames')\n",
    "#                         axs[0].set_title('Log mel spectrogram')\n",
    "#                         axs[1].set_title('Ground truth frames')\n",
    "#                         axs[2].set_title('frame_output')\n",
    "#                         axs[3].set_title('Ground truth reg onset')\n",
    "#                         axs[4].set_title('reg_onset_output')\n",
    "#                         axs[5].set_title('Ground truth reg offset')\n",
    "#                         axs[6].set_title('reg_offset_output')\n",
    "#                         plt.tight_layout(0, .05, 0)\n",
    "#                         fig_path = '_zz2.pdf'\n",
    "#                         plt.savefig(fig_path)\n",
    "#                         print('Plot to {}'.format(fig_path))\n",
    "\n",
    "\n",
    "#                         import crash\n",
    "#                         asdf\n",
    "\n",
    "#                 n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6624b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot3(args):\n",
    "    # \n",
    "    \"\"\"Inference the output probabilites of MAESTRO dataset.\n",
    "    Args:\n",
    "      cuda: bool\n",
    "      audio_path: str\n",
    "    \"\"\"\n",
    "\n",
    "    # Arugments & parameters\n",
    "    workspace = args.workspace\n",
    "    model_type = args.model_type\n",
    "    dataset = args.dataset\n",
    "    split = args.split\n",
    "    device = torch.device('cuda') if args.cuda and torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    sample_rate = config.sample_rate\n",
    "    segment_seconds = config.segment_seconds\n",
    "    segment_samples = int(segment_seconds * sample_rate)\n",
    "    frames_per_second = config.frames_per_second\n",
    "    classes_num = config.classes_num\n",
    "    begin_note = config.begin_note\n",
    "\n",
    "    # Paths\n",
    "    hdf5s_dir = os.path.join(workspace, 'hdf5s', dataset)\n",
    "\n",
    "    # Transcriptor\n",
    "    checkpoint_path = os.path.join(workspace, 'combined_models/google_note_pedal_random.pth')\n",
    "    # checkpoint_path = os.path.join(workspace, 'combined_models/google_note_pedal.pth')\n",
    "    transcriptor1 = PianoTranscription(model_type, device=device, \n",
    "        checkpoint_path=checkpoint_path, segment_samples=segment_samples, \n",
    "        post_processor_type='onsets_frames')\n",
    "\n",
    "    checkpoint_path = os.path.join(workspace, 'combined_models/regress_random_note_pedal.pth')\n",
    "    # checkpoint_path = os.path.join(workspace, 'combined_models/regress_note_pedal.pth')\n",
    "    transcriptor2 = PianoTranscription(model_type, device=device, \n",
    "        checkpoint_path=checkpoint_path, segment_samples=segment_samples, \n",
    "        post_processor_type='regression')\n",
    "\n",
    "    (hdf5_names, hdf5_paths) = traverse_folder(hdf5s_dir)\n",
    "\n",
    "    n = 0\n",
    "    for n, hdf5_path in enumerate(hdf5_paths):\n",
    "        with h5py.File(hdf5_path, 'r') as hf:\n",
    "            if hf.attrs['split'].decode() == split:\n",
    "                print(n, hdf5_path)\n",
    "                \n",
    "                if n == 90:\n",
    "                    # Load audio                \n",
    "                    audio = int16_to_float32(hf['waveform'][:])\n",
    "                    midi_events = [e.decode() for e in hf['midi_event'][:]]\n",
    "                    midi_events_time = hf['midi_event_time'][:]\n",
    "            \n",
    "                    # Ground truths processor\n",
    "                    target_processor = TargetProcessor(\n",
    "                        segment_seconds=len(audio) / sample_rate, \n",
    "                        frames_per_second=frames_per_second, begin_note=begin_note, \n",
    "                        classes_num=classes_num)\n",
    "\n",
    "                    # Get ground truths\n",
    "                    (target_dict, note_events, pedal_events) = \\\n",
    "                        target_processor.process(start_time=0, \n",
    "                            midi_events_time=midi_events_time, \n",
    "                            midi_events=midi_events, extend_pedal=True)\n",
    "\n",
    "                    ref_on_off_pairs = np.array([[event['onset_time'], event['offset_time']] for event in note_events])\n",
    "                    ref_midi_notes = np.array([event['midi_note'] for event in note_events])\n",
    "                    ref_velocity = np.array([event['velocity'] for event in note_events])\n",
    "\n",
    "                    # Transcribe\n",
    "                    transcribed_dict = transcriptor1.transcribe(audio, midi_path=None)\n",
    "                    output_dict = transcribed_dict['output_dict']\n",
    "\n",
    "                    # Pack probabilites to dump\n",
    "                    total_dict1 = {key: output_dict[key] for key in output_dict.keys()}\n",
    "                    total_dict1['frame_roll'] = target_dict['frame_roll']\n",
    "                    total_dict1['ref_on_off_pairs'] = ref_on_off_pairs\n",
    "                    total_dict1['ref_midi_notes'] = ref_midi_notes\n",
    "                    total_dict1['ref_velocity'] = ref_velocity\n",
    "\n",
    "                    if 'pedal_frame_output' in output_dict.keys():\n",
    "                        total_dict1['ref_pedal_on_off_pairs'] = \\\n",
    "                            np.array([[event['onset_time'], event['offset_time']] for event in pedal_events])\n",
    "                        total_dict1['pedal_frame_roll'] = target_dict['pedal_frame_roll']\n",
    "\n",
    "                    # Transcribe\n",
    "                    transcribed_dict = transcriptor2.transcribe(audio, midi_path=None)\n",
    "                    output_dict = transcribed_dict['output_dict']\n",
    "\n",
    "                    # Pack probabilites to dump\n",
    "                    total_dict2 = {key: output_dict[key] for key in output_dict.keys()}\n",
    "                    total_dict2['frame_roll'] = target_dict['frame_roll']\n",
    "                    total_dict2['ref_on_off_pairs'] = ref_on_off_pairs\n",
    "                    total_dict2['ref_midi_notes'] = ref_midi_notes\n",
    "                    total_dict2['ref_velocity'] = ref_velocity\n",
    "\n",
    "                    if 'pedal_frame_output' in output_dict.keys():\n",
    "                        total_dict2['ref_pedal_on_off_pairs'] = \\\n",
    "                            np.array([[event['onset_time'], event['offset_time']] for event in pedal_events])\n",
    "                        total_dict2['pedal_frame_roll'] = target_dict['pedal_frame_roll']\n",
    "\n",
    "                    if True:\n",
    "                        librosa.output.write_wav('_zz.wav', audio, sr=16000)\n",
    "                        bgn = 15500\n",
    "                        L = 500\n",
    "                        fontsize = 7\n",
    "                        vmin = None\n",
    "                        vmax = None\n",
    "\n",
    "                        import matplotlib.pyplot as plt\n",
    "                        fig, axs = plt.subplots(5, 1, figsize=(4, 4.5), sharex=True)\n",
    "                        mel = librosa.feature.melspectrogram(audio, sr=16000, n_fft=2048, hop_length=160, n_mels=229, fmin=30, fmax=8000).T\n",
    "                        axs[0].matshow(np.log(mel[bgn : bgn + L]).T, origin='lower', aspect='auto', cmap='jet')\n",
    "                        axs[1].matshow(total_dict1['reg_onset_output'][bgn : bgn + L].T, origin='lower', aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "                        axs[2].matshow(total_dict2['reg_onset_output'][bgn : bgn + L].T, origin='lower', aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "                        axs[3].matshow(total_dict1['reg_offset_output'][bgn : bgn + L].T, origin='lower', aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "                        axs[4].matshow(total_dict2['reg_offset_output'][bgn : bgn + L].T, origin='lower', aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "                        axs[0].set_xlim(0, len(total_dict1['frame_output'][bgn : bgn + L]))\n",
    "                        axs[0].set_title('Log mel spectrogram', fontsize=fontsize)\n",
    "                        axs[1].set_title('Google\\'s onsets output', fontsize=fontsize)\n",
    "                        axs[2].set_title('Regression onsets output', fontsize=fontsize)\n",
    "                        axs[3].set_title('Google\\'s offsets output', fontsize=fontsize)\n",
    "                        axs[4].set_title('Regression offsets output', fontsize=fontsize)\n",
    "                        axs[4].set_xlabel('Seconds')\n",
    "                        axs[0].yaxis.set_ticks(np.arange(0, 229, 228))\n",
    "                        axs[0].yaxis.set_ticklabels([0, 228], fontsize=fontsize)\n",
    "                        axs[0].set_ylabel('Mel bins', fontsize=fontsize)\n",
    "                        for i in range(4):\n",
    "                            axs[i].xaxis.set_ticks([])\n",
    "                            axs[i].xaxis.set_ticklabels([])\n",
    "                        for i in range(1, 5):\n",
    "                            axs[i].yaxis.set_ticks(np.arange(0, 88, 87))\n",
    "                            axs[i].yaxis.set_ticklabels([0, 87], fontsize=fontsize)\n",
    "                            axs[i].set_ylabel('Note', fontsize=fontsize)\n",
    "                        axs[4].xaxis.set_ticks([0, 100, 200, 300, 400, 499])\n",
    "                        axs[4].xaxis.set_ticklabels(['0', '1', '2', '3', '4', '5'], fontsize=fontsize)\n",
    "                        axs[4].set_xlabel('Seconds', fontsize=fontsize)\n",
    "                        axs[4].xaxis.set_ticks_position('bottom')\n",
    "                        plt.tight_layout(0, 0, 0)\n",
    "                        fig_path = '_zz.pdf'\n",
    "                        plt.savefig(fig_path)\n",
    "                        print('Plot to {}'.format(fig_path))\n",
    "\n",
    "                        import crash\n",
    "                        asdf\n",
    "\n",
    "                        fig, axs = plt.subplots(5, 1, figsize=(8, 8), sharex=True)\n",
    "                        mel = librosa.feature.melspectrogram(audio, sr=16000, n_fft=2048, hop_length=160, n_mels=229, fmin=30, fmax=8000).T\n",
    "                        axs[0].matshow(np.log(mel[bgn : bgn + L]).T, origin='lower', aspect='auto', cmap='jet')\n",
    "                        axs[1].plot(total_dict1['reg_pedal_offset_output'][bgn : bgn + L])\n",
    "                        axs[2].plot(total_dict2['reg_pedal_offset_output'][bgn : bgn + L])\n",
    "                        axs[0].set_xlim(0, len(output_dict['frame_output'][bgn : bgn + L]))\n",
    "                        axs[6].set_xlabel('Frames')\n",
    "                        axs[0].set_title('Log mel spectrogram')\n",
    "                        axs[1].set_title('Ground truth frames')\n",
    "                        axs[2].set_title('frame_output')\n",
    "                        axs[3].set_title('Ground truth reg onset')\n",
    "                        axs[4].set_title('reg_onset_output')\n",
    "                        axs[5].set_title('Ground truth reg offset')\n",
    "                        axs[6].set_title('reg_offset_output')\n",
    "                        plt.tight_layout(0, .05, 0)\n",
    "                        fig_path = '_zz2.pdf'\n",
    "                        plt.savefig(fig_path)\n",
    "                        print('Plot to {}'.format(fig_path))\n",
    "\n",
    "\n",
    "                        import crash\n",
    "                        asdf\n",
    "\n",
    "                n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43949c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_midi(args):\n",
    "\n",
    "    audio_path = args.audio_path\n",
    "    midi_path = args.midi_path\n",
    "    fig_path = 'results/{}.png'.format(get_filename(audio_path))\n",
    "\n",
    "    (audio, _) = librosa.core.load(audio_path, sr=config.sample_rate, mono=True)\n",
    "    audio_seconds = audio.shape[0] / config.sample_rate\n",
    "\n",
    "    midi_dict = read_midi(midi_path)\n",
    "\n",
    "    target_processor = TargetProcessor(segment_seconds=audio_seconds, \n",
    "        frames_per_second=config.frames_per_second, begin_note=config.begin_note, \n",
    "        classes_num=config.classes_num)\n",
    "\n",
    "    (target_dict, note_events, pedal_events) = target_processor.process(\n",
    "        start_time=0, \n",
    "        midi_events_time=midi_dict['midi_event_time'], \n",
    "        midi_events=midi_dict['midi_event'])\n",
    "    \n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 4), sharex=True)\n",
    "    logmel = np.log(librosa.feature.melspectrogram(audio, sr=16000, n_fft=2048, hop_length=160, n_mels=229, fmin=30, fmax=8000)).T\n",
    "    axs[0].matshow(logmel.T, origin='lower', aspect='auto', cmap='jet')\n",
    "    axs[1].matshow(target_dict['frame_roll'].T, origin='lower', aspect='auto', cmap='jet', vmin=-1, vmax=1)\n",
    "    axs[2].plot(target_dict['pedal_frame_roll'])\n",
    "    axs[2].set_ylim(-0.02, 1.02)\n",
    "    axs[0].set_title('Log mel spectrogram')\n",
    "    axs[1].set_title('Transcribed notes')\n",
    "    axs[2].set_title('Transcribed pedals')\n",
    "    axs[0].yaxis.set_ticks(np.arange(0, 229, 228))\n",
    "    axs[0].yaxis.set_ticklabels([0, 228])\n",
    "    axs[1].yaxis.set_ticks(np.arange(0, 88, 87))\n",
    "    axs[1].yaxis.set_ticklabels([0, 87])\n",
    "    axs[0].set_ylabel('Mel bins')\n",
    "    axs[1].set_ylabel('Notes')\n",
    "    axs[2].set_ylabel('Probability')\n",
    "    fps = config.frames_per_second\n",
    "    axs[2].xaxis.set_ticks(np.arange(0, audio_seconds * fps + 1, 5 * fps))\n",
    "    axs[2].xaxis.set_ticklabels(np.arange(0, audio_seconds + 1e-6, 5))\n",
    "    axs[2].set_xlabel('Seconds')\n",
    "    plt.tight_layout(0, 0, 0)\n",
    "    plt.savefig(fig_path)\n",
    "    print('Save out to {}'.format(fig_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab908dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] {plot,plot2,plot3,plot_midi} ...\n",
      "ipykernel_launcher.py: error: argument mode: invalid choice: '/root/.local/share/jupyter/runtime/kernel-f27b08ed-ece4-4022-8ac7-5a99f564a06c.json' (choose from 'plot', 'plot2', 'plot3', 'plot_midi')\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='')\n",
    "    subparsers = parser.add_subparsers(dest='mode')\n",
    "    \n",
    "    parser_plot = subparsers.add_parser('plot')\n",
    "    parser_plot.add_argument('--workspace', type=str, required=True)\n",
    "    parser_plot.add_argument('--model_type', type=str, required=True)\n",
    "    parser_plot.add_argument('--checkpoint_path', type=str, required=True)\n",
    "    parser_plot.add_argument('--dataset', type=str, required=True, choices=['maestro', 'maps'])\n",
    "    parser_plot.add_argument('--split', type=str, required=True)\n",
    "    parser_plot.add_argument('--post_processor_type', type=str, default='regression')\n",
    "    parser_plot.add_argument('--cuda', action='store_true', default=False)\n",
    "    \n",
    "    parser_plot = subparsers.add_parser('plot2')\n",
    "    parser_plot.add_argument('--workspace', type=str, required=True)\n",
    "    parser_plot.add_argument('--model_type', type=str, required=True)\n",
    "    parser_plot.add_argument('--checkpoint_path', type=str, required=True)\n",
    "    parser_plot.add_argument('--dataset', type=str, required=True, choices=['maestro', 'maps'])\n",
    "    parser_plot.add_argument('--split', type=str, required=True)\n",
    "    parser_plot.add_argument('--post_processor_type', type=str, default='regression')\n",
    "    parser_plot.add_argument('--cuda', action='store_true', default=False)\n",
    "\n",
    "    parser_plot = subparsers.add_parser('plot3')\n",
    "    parser_plot.add_argument('--workspace', type=str, required=True)\n",
    "    parser_plot.add_argument('--model_type', type=str, required=True)\n",
    "    parser_plot.add_argument('--dataset', type=str, required=True, choices=['maestro', 'maps'])\n",
    "    parser_plot.add_argument('--split', type=str, required=True)\n",
    "    parser_plot.add_argument('--cuda', action='store_true', default=False)\n",
    "\n",
    "    parser_plot = subparsers.add_parser('plot_midi')\n",
    "    parser_plot.add_argument('--audio_path', type=str, required=True)\n",
    "    parser_plot.add_argument('--midi_path', type=str, required=True)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.mode == 'plot':\n",
    "        plot(args)\n",
    "\n",
    "    elif args.mode == 'plot2':\n",
    "        plot2(args)\n",
    "\n",
    "    elif args.mode == 'plot3':\n",
    "        plot3(args)\n",
    "\n",
    "    elif args.mode == 'plot_midi':\n",
    "        plot_midi(args)\n",
    "\n",
    "    else:\n",
    "        raise Exception('Incorrct argument!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bab58f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
