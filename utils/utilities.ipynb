{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b078975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import h5py\n",
    "import soundfile\n",
    "import librosa\n",
    "import audioread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "import collections\n",
    "import pickle\n",
    "from mido import MidiFile\n",
    "\n",
    "from piano_vad import (note_detection_with_onset_offset_regress, \n",
    "    pedal_detection_with_onset_offset_regress, onsets_frames_note_detection, onsets_frames_pedal_detection)\n",
    "import config\n",
    "\n",
    "\n",
    "def create_folder(fd):\n",
    "    if not os.path.exists(fd):\n",
    "        os.makedirs(fd)\n",
    "        \n",
    "        \n",
    "def get_filename(path):\n",
    "    path = os.path.realpath(path)\n",
    "    na_ext = path.split('/')[-1]\n",
    "    na = os.path.splitext(na_ext)[0]\n",
    "    return na\n",
    "\n",
    "\n",
    "def traverse_folder(folder):\n",
    "    paths = []\n",
    "    names = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for name in files:\n",
    "            filepath = os.path.join(root, name)\n",
    "            names.append(name)\n",
    "            paths.append(filepath)\n",
    "            \n",
    "    return names, paths\n",
    "\n",
    "\n",
    "def note_to_freq(piano_note):\n",
    "    return 2 ** ((piano_note - 39) / 12) * 440\n",
    "\n",
    "    \n",
    "def create_logging(log_dir, filemode):\n",
    "    create_folder(log_dir)\n",
    "    i1 = 0\n",
    "\n",
    "    while os.path.isfile(os.path.join(log_dir, '{:04d}.log'.format(i1))):\n",
    "        i1 += 1\n",
    "        \n",
    "    log_path = os.path.join(log_dir, '{:04d}.log'.format(i1))\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG,\n",
    "        format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',\n",
    "        datefmt='%a, %d %b %Y %H:%M:%S',\n",
    "        filename=log_path,\n",
    "        filemode=filemode)\n",
    "\n",
    "    # Print to console\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')\n",
    "    console.setFormatter(formatter)\n",
    "    logging.getLogger('').addHandler(console)\n",
    "    \n",
    "    return logging\n",
    "\n",
    "\n",
    "def float32_to_int16(x):\n",
    "    assert np.max(np.abs(x)) <= 1.\n",
    "    return (x * 32767.).astype(np.int16)\n",
    "\n",
    "\n",
    "def int16_to_float32(x):\n",
    "    return (x / 32767.).astype(np.float32)\n",
    "    \n",
    "\n",
    "def pad_truncate_sequence(x, max_len):\n",
    "    if len(x) < max_len:\n",
    "        return np.concatenate((x, np.zeros(max_len - len(x))))\n",
    "    else:\n",
    "        return x[0 : max_len]\n",
    "\n",
    "\n",
    "def read_metadata(csv_path):\n",
    "    \"\"\"Read metadata of MAESTRO dataset from csv file.\n",
    "    Args:\n",
    "      csv_path: str\n",
    "    Returns:\n",
    "      meta_dict, dict, e.g. {\n",
    "        'canonical_composer': ['Alban Berg', ...], \n",
    "        'canonical_title': ['Sonata Op. 1', ...], \n",
    "        'split': ['train', ...], \n",
    "        'year': ['2018', ...]\n",
    "        'midi_filename': ['2018/MIDI-Unprocessed_Chamber3_MID--AUDIO_10_R3_2018_wav--1.midi', ...], \n",
    "        'audio_filename': ['2018/MIDI-Unprocessed_Chamber3_MID--AUDIO_10_R3_2018_wav--1.wav', ...],\n",
    "        'duration': [698.66116031, ...]}\n",
    "    \"\"\"\n",
    "\n",
    "    with open(csv_path, 'r') as fr:\n",
    "        reader = csv.reader(fr, delimiter=',')\n",
    "        lines = list(reader)\n",
    "\n",
    "    meta_dict = {'canonical_composer': [], 'canonical_title': [], 'split': [], \n",
    "        'year': [], 'midi_filename': [], 'audio_filename': [], 'duration': []}\n",
    "\n",
    "    for n in range(1, len(lines)):\n",
    "        meta_dict['canonical_composer'].append(lines[n][0])\n",
    "        meta_dict['canonical_title'].append(lines[n][1])\n",
    "        meta_dict['split'].append(lines[n][2])\n",
    "        meta_dict['year'].append(lines[n][3])\n",
    "        meta_dict['midi_filename'].append(lines[n][4])\n",
    "        meta_dict['audio_filename'].append(lines[n][5])\n",
    "        meta_dict['duration'].append(float(lines[n][6]))\n",
    "\n",
    "    for key in meta_dict.keys():\n",
    "        meta_dict[key] = np.array(meta_dict[key])\n",
    "    \n",
    "    return meta_dict\n",
    "\n",
    "\n",
    "def read_midi(midi_path):\n",
    "    \"\"\"Parse MIDI file.\n",
    "    Args:\n",
    "      midi_path: str\n",
    "    Returns:\n",
    "      midi_dict: dict, e.g. {\n",
    "        'midi_event': [\n",
    "            'program_change channel=0 program=0 time=0', \n",
    "            'control_change channel=0 control=64 value=127 time=0', \n",
    "            'control_change channel=0 control=64 value=63 time=236', \n",
    "            ...],\n",
    "        'midi_event_time': [0., 0, 0.98307292, ...]}\n",
    "    \"\"\"\n",
    "\n",
    "    midi_file = MidiFile(midi_path)\n",
    "    ticks_per_beat = midi_file.ticks_per_beat\n",
    "\n",
    "    assert len(midi_file.tracks) == 2\n",
    "    \"\"\"The first track contains tempo, time signature. The second track \n",
    "    contains piano events.\"\"\"\n",
    "\n",
    "    microseconds_per_beat = midi_file.tracks[0][0].tempo\n",
    "    beats_per_second = 1e6 / microseconds_per_beat\n",
    "    ticks_per_second = ticks_per_beat * beats_per_second\n",
    "\n",
    "    message_list = []\n",
    "\n",
    "    ticks = 0\n",
    "    time_in_second = []\n",
    "\n",
    "    for message in midi_file.tracks[1]:\n",
    "        message_list.append(str(message))\n",
    "        ticks += message.time\n",
    "        time_in_second.append(ticks / ticks_per_second)\n",
    "\n",
    "    midi_dict = {\n",
    "        'midi_event': np.array(message_list), \n",
    "        'midi_event_time': np.array(time_in_second)}\n",
    "\n",
    "    return midi_dict\n",
    "\n",
    "\n",
    "def read_maps_midi(midi_path):\n",
    "    \"\"\"Parse MIDI file of MAPS dataset. Not used anymore.\n",
    "    Args:\n",
    "      midi_path: str\n",
    "    Returns:\n",
    "      midi_dict: dict, e.g. {\n",
    "        'midi_event': [\n",
    "            '<meta message set_tempo tempo=439440 time=0>',\n",
    "            'control_change channel=0 control=64 value=0 time=0',\n",
    "            'control_change channel=0 control=64 value=0 time=7531',\n",
    "            ...],\n",
    "        'midi_event_time': [0., 0.53200309, 0.53200309, ...]}\n",
    "    \"\"\"\n",
    "\n",
    "    midi_file = MidiFile(midi_path)\n",
    "    ticks_per_beat = midi_file.ticks_per_beat\n",
    "\n",
    "    assert len(midi_file.tracks) == 1\n",
    "\n",
    "    microseconds_per_beat = midi_file.tracks[0][0].tempo\n",
    "    beats_per_second = 1e6 / microseconds_per_beat\n",
    "    ticks_per_second = ticks_per_beat * beats_per_second\n",
    "\n",
    "    message_list = []\n",
    "\n",
    "    ticks = 0\n",
    "    time_in_second = []\n",
    "\n",
    "    for message in midi_file.tracks[0]:\n",
    "        message_list.append(str(message))\n",
    "        ticks += message.time\n",
    "        time_in_second.append(ticks / ticks_per_second)\n",
    "\n",
    "    midi_dict = {\n",
    "        'midi_event': np.array(message_list), \n",
    "        'midi_event_time': np.array(time_in_second)}\n",
    "\n",
    "    return midi_dict\n",
    "\n",
    "\n",
    "class TargetProcessor(object):\n",
    "    def __init__(self, segment_seconds, frames_per_second, begin_note, \n",
    "        classes_num):\n",
    "        \"\"\"Class for processing MIDI events to target.\n",
    "        Args:\n",
    "          segment_seconds: float\n",
    "          frames_per_second: int\n",
    "          begin_note: int, A0 MIDI note of a piano\n",
    "          classes_num: int\n",
    "        \"\"\"\n",
    "        self.segment_seconds = segment_seconds\n",
    "        self.frames_per_second = frames_per_second\n",
    "        self.begin_note = begin_note\n",
    "        self.classes_num = classes_num\n",
    "        self.max_piano_note = self.classes_num - 1\n",
    "\n",
    "    def process(self, start_time, midi_events_time, midi_events, \n",
    "        extend_pedal=True, note_shift=0):\n",
    "        \"\"\"Process MIDI events of an audio segment to target for training, \n",
    "        includes: \n",
    "        1. Parse MIDI events\n",
    "        2. Prepare note targets\n",
    "        3. Prepare pedal targets\n",
    "        Args:\n",
    "          start_time: float, start time of a segment\n",
    "          midi_events_time: list of float, times of MIDI events of a recording, \n",
    "            e.g. [0, 3.3, 5.1, ...]\n",
    "          midi_events: list of str, MIDI events of a recording, e.g.\n",
    "            ['note_on channel=0 note=75 velocity=37 time=14',\n",
    "             'control_change channel=0 control=64 value=54 time=20',\n",
    "             ...]\n",
    "          extend_pedal, bool, True: Notes will be set to ON until pedal is \n",
    "            released. False: Ignore pedal events.\n",
    "        Returns:\n",
    "          target_dict: {\n",
    "            'onset_roll': (frames_num, classes_num), \n",
    "            'offset_roll': (frames_num, classes_num), \n",
    "            'reg_onset_roll': (frames_num, classes_num), \n",
    "            'reg_offset_roll': (frames_num, classes_num), \n",
    "            'frame_roll': (frames_num, classes_num), \n",
    "            'velocity_roll': (frames_num, classes_num), \n",
    "            'mask_roll':  (frames_num, classes_num), \n",
    "            'pedal_onset_roll': (frames_num,), \n",
    "            'pedal_offset_roll': (frames_num,), \n",
    "            'reg_pedal_onset_roll': (frames_num,), \n",
    "            'reg_pedal_offset_roll': (frames_num,), \n",
    "            'pedal_frame_roll': (frames_num,)}\n",
    "          note_events: list of dict, e.g. [\n",
    "            {'midi_note': 51, 'onset_time': 696.64, 'offset_time': 697.00, 'velocity': 44}, \n",
    "            {'midi_note': 58, 'onset_time': 697.00, 'offset_time': 697.19, 'velocity': 50}\n",
    "            ...]\n",
    "          pedal_events: list of dict, e.g. [\n",
    "            {'onset_time': 149.37, 'offset_time': 150.35}, \n",
    "            {'onset_time': 150.54, 'offset_time': 152.06}, \n",
    "            ...]\n",
    "        \"\"\"\n",
    "\n",
    "        # ------ 1. Parse MIDI events ------\n",
    "        # Search the begin index of a segment\n",
    "        for bgn_idx, event_time in enumerate(midi_events_time):\n",
    "            if event_time > start_time:\n",
    "                break\n",
    "        \"\"\"E.g., start_time: 709.0, bgn_idx: 18003, event_time: 709.0146\"\"\"\n",
    "\n",
    "        # Search the end index of a segment\n",
    "        for fin_idx, event_time in enumerate(midi_events_time):\n",
    "            if event_time > start_time + self.segment_seconds:\n",
    "                break\n",
    "        \"\"\"E.g., start_time: 709.0, bgn_idx: 18196, event_time: 719.0115\"\"\"\n",
    "\n",
    "        note_events = []\n",
    "        \"\"\"E.g. [\n",
    "            {'midi_note': 51, 'onset_time': 696.63544, 'offset_time': 696.9948, 'velocity': 44}, \n",
    "            {'midi_note': 58, 'onset_time': 696.99585, 'offset_time': 697.18646, 'velocity': 50}\n",
    "            ...]\"\"\"\n",
    "\n",
    "        pedal_events = []\n",
    "        \"\"\"E.g. [\n",
    "            {'onset_time': 696.46875, 'offset_time': 696.62604}, \n",
    "            {'onset_time': 696.8063, 'offset_time': 698.50836}, \n",
    "            ...]\"\"\"\n",
    "\n",
    "        buffer_dict = {}    # Used to store onset of notes to be paired with offsets\n",
    "        pedal_dict = {}     # Used to store onset of pedal to be paired with offset of pedal\n",
    "\n",
    "        # Backtrack bgn_idx to earlier indexes: ex_bgn_idx, which is used for \n",
    "        # searching cross segment pedal and note events. E.g.: bgn_idx: 1149, \n",
    "        # ex_bgn_idx: 981\n",
    "        _delta = int((fin_idx - bgn_idx) * 1.)  \n",
    "        ex_bgn_idx = max(bgn_idx - _delta, 0)\n",
    "        \n",
    "        for i in range(ex_bgn_idx, fin_idx):\n",
    "            # Parse MIDI messiage\n",
    "            attribute_list = midi_events[i].split(' ')\n",
    "\n",
    "            # Note\n",
    "            if attribute_list[0] in ['note_on', 'note_off']:\n",
    "                \"\"\"E.g. attribute_list: ['note_on', 'channel=0', 'note=41', 'velocity=0', 'time=10']\"\"\"\n",
    "\n",
    "                midi_note = int(attribute_list[2].split('=')[1])\n",
    "                velocity = int(attribute_list[3].split('=')[1])\n",
    "\n",
    "                # Onset\n",
    "                if attribute_list[0] == 'note_on' and velocity > 0:\n",
    "                    buffer_dict[midi_note] = {\n",
    "                        'onset_time': midi_events_time[i], \n",
    "                        'velocity': velocity}\n",
    "\n",
    "                # Offset\n",
    "                else:\n",
    "                    if midi_note in buffer_dict.keys():\n",
    "                        note_events.append({\n",
    "                            'midi_note': midi_note, \n",
    "                            'onset_time': buffer_dict[midi_note]['onset_time'], \n",
    "                            'offset_time': midi_events_time[i], \n",
    "                            'velocity': buffer_dict[midi_note]['velocity']})\n",
    "                        del buffer_dict[midi_note]\n",
    "\n",
    "            # Pedal\n",
    "            elif attribute_list[0] == 'control_change' and attribute_list[2] == 'control=64':\n",
    "                \"\"\"control=64 corresponds to pedal MIDI event. E.g. \n",
    "                attribute_list: ['control_change', 'channel=0', 'control=64', 'value=45', 'time=43']\"\"\"\n",
    "\n",
    "                ped_value = int(attribute_list[3].split('=')[1])\n",
    "                if ped_value >= 64:\n",
    "                    if 'onset_time' not in pedal_dict:\n",
    "                        pedal_dict['onset_time'] = midi_events_time[i]\n",
    "                else:\n",
    "                    if 'onset_time' in pedal_dict:\n",
    "                        pedal_events.append({\n",
    "                            'onset_time': pedal_dict['onset_time'], \n",
    "                            'offset_time': midi_events_time[i]})\n",
    "                        pedal_dict = {}\n",
    "\n",
    "        # Add unpaired onsets to events\n",
    "        for midi_note in buffer_dict.keys():\n",
    "            note_events.append({\n",
    "                'midi_note': midi_note, \n",
    "                'onset_time': buffer_dict[midi_note]['onset_time'], \n",
    "                'offset_time': start_time + self.segment_seconds, \n",
    "                'velocity': buffer_dict[midi_note]['velocity']})\n",
    "\n",
    "        # Add unpaired pedal onsets to data\n",
    "        if 'onset_time' in pedal_dict.keys():\n",
    "            pedal_events.append({\n",
    "                'onset_time': pedal_dict['onset_time'], \n",
    "                'offset_time': start_time + self.segment_seconds})\n",
    "\n",
    "        # Set notes to ON until pedal is released\n",
    "        if extend_pedal:\n",
    "            note_events = self.extend_pedal(note_events, pedal_events)\n",
    "        \n",
    "        # Prepare targets\n",
    "        frames_num = int(round(self.segment_seconds * self.frames_per_second)) + 1\n",
    "        onset_roll = np.zeros((frames_num, self.classes_num))\n",
    "        offset_roll = np.zeros((frames_num, self.classes_num))\n",
    "        reg_onset_roll = np.ones((frames_num, self.classes_num))\n",
    "        reg_offset_roll = np.ones((frames_num, self.classes_num))\n",
    "        frame_roll = np.zeros((frames_num, self.classes_num))\n",
    "        velocity_roll = np.zeros((frames_num, self.classes_num))\n",
    "        mask_roll = np.ones((frames_num, self.classes_num))\n",
    "        \"\"\"mask_roll is used for masking out cross segment notes\"\"\"\n",
    "\n",
    "        pedal_onset_roll = np.zeros(frames_num)\n",
    "        pedal_offset_roll = np.zeros(frames_num)\n",
    "        reg_pedal_onset_roll = np.ones(frames_num)\n",
    "        reg_pedal_offset_roll = np.ones(frames_num)\n",
    "        pedal_frame_roll = np.zeros(frames_num)\n",
    "\n",
    "        # ------ 2. Get note targets ------\n",
    "        # Process note events to target\n",
    "        for note_event in note_events:\n",
    "            \"\"\"note_event: e.g., {'midi_note': 60, 'onset_time': 722.0719, 'offset_time': 722.47815, 'velocity': 103}\"\"\"\n",
    "\n",
    "            piano_note = np.clip(note_event['midi_note'] - self.begin_note + note_shift, 0, self.max_piano_note) \n",
    "            \"\"\"There are 88 keys on a piano\"\"\"\n",
    "\n",
    "            if 0 <= piano_note <= self.max_piano_note:\n",
    "                bgn_frame = int(round((note_event['onset_time'] - start_time) * self.frames_per_second))\n",
    "                fin_frame = int(round((note_event['offset_time'] - start_time) * self.frames_per_second))\n",
    "\n",
    "                if fin_frame >= 0:\n",
    "                    frame_roll[max(bgn_frame, 0) : fin_frame + 1, piano_note] = 1\n",
    "\n",
    "                    offset_roll[fin_frame, piano_note] = 1\n",
    "                    velocity_roll[max(bgn_frame, 0) : fin_frame + 1, piano_note] = note_event['velocity']\n",
    "\n",
    "                    # Vector from the center of a frame to ground truth offset\n",
    "                    reg_offset_roll[fin_frame, piano_note] = \\\n",
    "                        (note_event['offset_time'] - start_time) - (fin_frame / self.frames_per_second)\n",
    "\n",
    "                    if bgn_frame >= 0:\n",
    "                        onset_roll[bgn_frame, piano_note] = 1\n",
    "\n",
    "                        # Vector from the center of a frame to ground truth onset\n",
    "                        reg_onset_roll[bgn_frame, piano_note] = \\\n",
    "                            (note_event['onset_time'] - start_time) - (bgn_frame / self.frames_per_second)\n",
    "                \n",
    "                    # Mask out segment notes\n",
    "                    else:\n",
    "                        mask_roll[: fin_frame + 1, piano_note] = 0\n",
    "\n",
    "        for k in range(self.classes_num):\n",
    "            \"\"\"Get regression targets\"\"\"\n",
    "            reg_onset_roll[:, k] = self.get_regression(reg_onset_roll[:, k])\n",
    "            reg_offset_roll[:, k] = self.get_regression(reg_offset_roll[:, k])\n",
    "\n",
    "        # Process unpaired onsets to target\n",
    "        for midi_note in buffer_dict.keys():\n",
    "            piano_note = np.clip(midi_note - self.begin_note + note_shift, 0, self.max_piano_note)\n",
    "            if 0 <= piano_note <= self.max_piano_note:\n",
    "                bgn_frame = int(round((buffer_dict[midi_note]['onset_time'] - start_time) * self.frames_per_second))\n",
    "                mask_roll[bgn_frame :, piano_note] = 0     \n",
    "\n",
    "        # ------ 3. Get pedal targets ------\n",
    "        # Process pedal events to target\n",
    "        for pedal_event in pedal_events:\n",
    "            bgn_frame = int(round((pedal_event['onset_time'] - start_time) * self.frames_per_second))\n",
    "            fin_frame = int(round((pedal_event['offset_time'] - start_time) * self.frames_per_second))\n",
    "\n",
    "            if fin_frame >= 0:\n",
    "                pedal_frame_roll[max(bgn_frame, 0) : fin_frame + 1] = 1\n",
    "\n",
    "                pedal_offset_roll[fin_frame] = 1\n",
    "                reg_pedal_offset_roll[fin_frame] = \\\n",
    "                    (pedal_event['offset_time'] - start_time) - (fin_frame / self.frames_per_second)\n",
    "\n",
    "                if bgn_frame >= 0:\n",
    "                    pedal_onset_roll[bgn_frame] = 1\n",
    "                    reg_pedal_onset_roll[bgn_frame] = \\\n",
    "                        (pedal_event['onset_time'] - start_time) - (bgn_frame / self.frames_per_second)\n",
    "\n",
    "        # Get regresssion padal targets\n",
    "        reg_pedal_onset_roll = self.get_regression(reg_pedal_onset_roll)\n",
    "        reg_pedal_offset_roll = self.get_regression(reg_pedal_offset_roll)\n",
    "\n",
    "        target_dict = {\n",
    "            'onset_roll': onset_roll, 'offset_roll': offset_roll,\n",
    "            'reg_onset_roll': reg_onset_roll, 'reg_offset_roll': reg_offset_roll,\n",
    "            'frame_roll': frame_roll, 'velocity_roll': velocity_roll, \n",
    "            'mask_roll': mask_roll, 'reg_pedal_onset_roll': reg_pedal_onset_roll, \n",
    "            'pedal_onset_roll': pedal_onset_roll, 'pedal_offset_roll': pedal_offset_roll, \n",
    "            'reg_pedal_offset_roll': reg_pedal_offset_roll, 'pedal_frame_roll': pedal_frame_roll\n",
    "            }\n",
    "\n",
    "        return target_dict, note_events, pedal_events\n",
    "\n",
    "    def extend_pedal(self, note_events, pedal_events):\n",
    "        \"\"\"Update the offset of all notes until pedal is released.\n",
    "        Args:\n",
    "          note_events: list of dict, e.g., [\n",
    "            {'midi_note': 51, 'onset_time': 696.63544, 'offset_time': 696.9948, 'velocity': 44}, \n",
    "            {'midi_note': 58, 'onset_time': 696.99585, 'offset_time': 697.18646, 'velocity': 50}\n",
    "            ...]\n",
    "          pedal_events: list of dict, e.g., [\n",
    "            {'onset_time': 696.46875, 'offset_time': 696.62604}, \n",
    "            {'onset_time': 696.8063, 'offset_time': 698.50836}, \n",
    "            ...]\n",
    "        Returns:\n",
    "          ex_note_events: list of dict, e.g., [\n",
    "            {'midi_note': 51, 'onset_time': 696.63544, 'offset_time': 696.9948, 'velocity': 44}, \n",
    "            {'midi_note': 58, 'onset_time': 696.99585, 'offset_time': 697.18646, 'velocity': 50}\n",
    "            ...]\n",
    "        \"\"\"\n",
    "        note_events = collections.deque(note_events)\n",
    "        pedal_events = collections.deque(pedal_events)\n",
    "        ex_note_events = []\n",
    "\n",
    "        idx = 0     # Index of note events\n",
    "        while pedal_events: # Go through all pedal events\n",
    "            pedal_event = pedal_events.popleft()\n",
    "            buffer_dict = {}    # keys: midi notes, value for each key: event index\n",
    "\n",
    "            while note_events:\n",
    "                note_event = note_events.popleft()\n",
    "\n",
    "                # If a note offset is between the onset and offset of a pedal, \n",
    "                # Then set the note offset to when the pedal is released.\n",
    "                if pedal_event['onset_time'] < note_event['offset_time'] < pedal_event['offset_time']:\n",
    "                    \n",
    "                    midi_note = note_event['midi_note']\n",
    "\n",
    "                    if midi_note in buffer_dict.keys():\n",
    "                        \"\"\"Multiple same note inside a pedal\"\"\"\n",
    "                        _idx = buffer_dict[midi_note]\n",
    "                        del buffer_dict[midi_note]\n",
    "                        ex_note_events[_idx]['offset_time'] = note_event['onset_time']\n",
    "\n",
    "                    # Set note offset to pedal offset\n",
    "                    note_event['offset_time'] = pedal_event['offset_time']\n",
    "                    buffer_dict[midi_note] = idx\n",
    "                \n",
    "                ex_note_events.append(note_event)\n",
    "                idx += 1\n",
    "\n",
    "                # Break loop and pop next pedal\n",
    "                if note_event['offset_time'] > pedal_event['offset_time']:\n",
    "                    break\n",
    "\n",
    "        while note_events:\n",
    "            \"\"\"Append left notes\"\"\"\n",
    "            ex_note_events.append(note_events.popleft())\n",
    "\n",
    "        return ex_note_events\n",
    "\n",
    "    def get_regression(self, input):\n",
    "        \"\"\"Get regression target. See Fig. 2 of [1] for an example.\n",
    "        [1] Q. Kong, et al., High-resolution Piano Transcription with Pedals by \n",
    "        Regressing Onsets and Offsets Times, 2020.\n",
    "        input:\n",
    "          input: (frames_num,)\n",
    "        Returns: (frames_num,), e.g., [0, 0, 0.1, 0.3, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.3, 0.1, 0, 0, ...]\n",
    "        \"\"\"\n",
    "        step = 1. / self.frames_per_second\n",
    "        output = np.ones_like(input)\n",
    "        \n",
    "        locts = np.where(input < 0.5)[0] \n",
    "        if len(locts) > 0:\n",
    "            for t in range(0, locts[0]):\n",
    "                output[t] = step * (t - locts[0]) - input[locts[0]]\n",
    "\n",
    "            for i in range(0, len(locts) - 1):\n",
    "                for t in range(locts[i], (locts[i] + locts[i + 1]) // 2):\n",
    "                    output[t] = step * (t - locts[i]) - input[locts[i]]\n",
    "\n",
    "                for t in range((locts[i] + locts[i + 1]) // 2, locts[i + 1]):\n",
    "                    output[t] = step * (t - locts[i + 1]) - input[locts[i]]\n",
    "\n",
    "            for t in range(locts[-1], len(input)):\n",
    "                output[t] = step * (t - locts[-1]) - input[locts[-1]]\n",
    "\n",
    "        output = np.clip(np.abs(output), 0., 0.05) * 20\n",
    "        output = (1. - output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def write_events_to_midi(start_time, note_events, pedal_events, midi_path):\n",
    "    \"\"\"Write out note events to MIDI file.\n",
    "    Args:\n",
    "      start_time: float\n",
    "      note_events: list of dict, e.g. [\n",
    "        {'midi_note': 51, 'onset_time': 696.63544, 'offset_time': 696.9948, 'velocity': 44}, \n",
    "        {'midi_note': 58, 'onset_time': 696.99585, 'offset_time': 697.18646, 'velocity': 50}\n",
    "        ...]\n",
    "      midi_path: str\n",
    "    \"\"\"\n",
    "    from mido import Message, MidiFile, MidiTrack, MetaMessage\n",
    "    \n",
    "    # This configuration is the same as MIDIs in MAESTRO dataset\n",
    "    ticks_per_beat = 384\n",
    "    beats_per_second = 2\n",
    "    ticks_per_second = ticks_per_beat * beats_per_second\n",
    "    microseconds_per_beat = int(1e6 // beats_per_second)\n",
    "\n",
    "    midi_file = MidiFile()\n",
    "    midi_file.ticks_per_beat = ticks_per_beat\n",
    "\n",
    "    # Track 0\n",
    "    track0 = MidiTrack()\n",
    "    track0.append(MetaMessage('set_tempo', tempo=microseconds_per_beat, time=0))\n",
    "    track0.append(MetaMessage('time_signature', numerator=4, denominator=4, time=0))\n",
    "    track0.append(MetaMessage('end_of_track', time=1))\n",
    "    midi_file.tracks.append(track0)\n",
    "\n",
    "    # Track 1\n",
    "    track1 = MidiTrack()\n",
    "    \n",
    "    # Message rolls of MIDI\n",
    "    message_roll = []\n",
    "\n",
    "    for note_event in note_events:\n",
    "        # Onset\n",
    "        message_roll.append({\n",
    "            'time': note_event['onset_time'], \n",
    "            'midi_note': note_event['midi_note'], \n",
    "            'velocity': note_event['velocity']})\n",
    "\n",
    "        # Offset\n",
    "        message_roll.append({\n",
    "            'time': note_event['offset_time'], \n",
    "            'midi_note': note_event['midi_note'], \n",
    "            'velocity': 0})\n",
    "\n",
    "    if pedal_events:\n",
    "        for pedal_event in pedal_events:\n",
    "            message_roll.append({'time': pedal_event['onset_time'], 'control_change': 64, 'value': 127})\n",
    "            message_roll.append({'time': pedal_event['offset_time'], 'control_change': 64, 'value': 0})\n",
    "\n",
    "    # Sort MIDI messages by time\n",
    "    message_roll.sort(key=lambda note_event: note_event['time'])\n",
    "\n",
    "    previous_ticks = 0\n",
    "    for message in message_roll:\n",
    "        this_ticks = int((message['time'] - start_time) * ticks_per_second)\n",
    "        if this_ticks >= 0:\n",
    "            diff_ticks = this_ticks - previous_ticks\n",
    "            previous_ticks = this_ticks\n",
    "            if 'midi_note' in message.keys():\n",
    "                track1.append(Message('note_on', note=message['midi_note'], velocity=message['velocity'], time=diff_ticks))\n",
    "            elif 'control_change' in message.keys():\n",
    "                track1.append(Message('control_change', channel=0, control=message['control_change'], value=message['value'], time=diff_ticks))\n",
    "    track1.append(MetaMessage('end_of_track', time=1))\n",
    "    midi_file.tracks.append(track1)\n",
    "\n",
    "    midi_file.save(midi_path)\n",
    "\n",
    "\n",
    "def plot_waveform_midi_targets(data_dict, start_time, note_events):\n",
    "    \"\"\"For debugging. Write out waveform, MIDI and plot targets for an \n",
    "    audio segment.\n",
    "    Args:\n",
    "      data_dict: {\n",
    "        'waveform': (samples_num,),\n",
    "        'onset_roll': (frames_num, classes_num), \n",
    "        'offset_roll': (frames_num, classes_num), \n",
    "        'reg_onset_roll': (frames_num, classes_num), \n",
    "        'reg_offset_roll': (frames_num, classes_num), \n",
    "        'frame_roll': (frames_num, classes_num), \n",
    "        'velocity_roll': (frames_num, classes_num), \n",
    "        'mask_roll':  (frames_num, classes_num), \n",
    "        'reg_pedal_onset_roll': (frames_num,),\n",
    "        'reg_pedal_offset_roll': (frames_num,),\n",
    "        'pedal_frame_roll': (frames_num,)}\n",
    "      start_time: float\n",
    "      note_events: list of dict, e.g. [\n",
    "        {'midi_note': 51, 'onset_time': 696.63544, 'offset_time': 696.9948, 'velocity': 44}, \n",
    "        {'midi_note': 58, 'onset_time': 696.99585, 'offset_time': 697.18646, 'velocity': 50}\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    create_folder('debug')\n",
    "    audio_path = 'debug/debug.wav'\n",
    "    midi_path = 'debug/debug.mid'\n",
    "    fig_path = 'debug/debug.pdf'\n",
    "\n",
    "    librosa.output.write_wav(audio_path, data_dict['waveform'], sr=config.sample_rate)\n",
    "    write_events_to_midi(start_time, note_events, midi_path)\n",
    "    x = librosa.core.stft(y=data_dict['waveform'], n_fft=2048, hop_length=160, window='hann', center=True)\n",
    "    x = np.abs(x) ** 2\n",
    "\n",
    "    fig, axs = plt.subplots(11, 1, sharex=True, figsize=(30, 30))\n",
    "    fontsize = 20\n",
    "    axs[0].matshow(np.log(x), origin='lower', aspect='auto', cmap='jet')\n",
    "    axs[1].matshow(data_dict['onset_roll'].T, origin='lower', aspect='auto', cmap='jet')\n",
    "    axs[2].matshow(data_dict['offset_roll'].T, origin='lower', aspect='auto', cmap='jet')\n",
    "    axs[3].matshow(data_dict['reg_onset_roll'].T, origin='lower', aspect='auto', cmap='jet')\n",
    "    axs[4].matshow(data_dict['reg_offset_roll'].T, origin='lower', aspect='auto', cmap='jet')\n",
    "    axs[5].matshow(data_dict['frame_roll'].T, origin='lower', aspect='auto', cmap='jet')\n",
    "    axs[6].matshow(data_dict['velocity_roll'].T, origin='lower', aspect='auto', cmap='jet')\n",
    "    axs[7].matshow(data_dict['mask_roll'].T, origin='lower', aspect='auto', cmap='jet')\n",
    "    axs[8].matshow(data_dict['reg_pedal_onset_roll'][:, None].T, origin='lower', aspect='auto', cmap='jet')\n",
    "    axs[9].matshow(data_dict['reg_pedal_offset_roll'][:, None].T, origin='lower', aspect='auto', cmap='jet')\n",
    "    axs[10].matshow(data_dict['pedal_frame_roll'][:, None].T, origin='lower', aspect='auto', cmap='jet')\n",
    "    axs[0].set_title('Log spectrogram', fontsize=fontsize)\n",
    "    axs[1].set_title('onset_roll', fontsize=fontsize)\n",
    "    axs[2].set_title('offset_roll', fontsize=fontsize)\n",
    "    axs[3].set_title('reg_onset_roll', fontsize=fontsize)\n",
    "    axs[4].set_title('reg_offset_roll', fontsize=fontsize)\n",
    "    axs[5].set_title('frame_roll', fontsize=fontsize)\n",
    "    axs[6].set_title('velocity_roll', fontsize=fontsize)\n",
    "    axs[7].set_title('mask_roll', fontsize=fontsize)\n",
    "    axs[8].set_title('reg_pedal_onset_roll', fontsize=fontsize)\n",
    "    axs[9].set_title('reg_pedal_offset_roll', fontsize=fontsize)\n",
    "    axs[10].set_title('pedal_frame_roll', fontsize=fontsize)\n",
    "    axs[10].set_xlabel('frames')\n",
    "    axs[10].xaxis.set_label_position('bottom')\n",
    "    axs[10].xaxis.set_ticks_position('bottom')\n",
    "    plt.tight_layout(1, 1, 1)\n",
    "    plt.savefig(fig_path)\n",
    "\n",
    "    print('Write out to {}, {}, {}!'.format(audio_path, midi_path, fig_path))\n",
    "\n",
    "\n",
    "class RegressionPostProcessor(object):\n",
    "    def __init__(self, frames_per_second, classes_num, onset_threshold, \n",
    "        offset_threshold, frame_threshold, pedal_offset_threshold):\n",
    "        \"\"\"Postprocess the output probabilities of a transription model to MIDI \n",
    "        events.\n",
    "        Args:\n",
    "          frames_per_second: int\n",
    "          classes_num: int\n",
    "          onset_threshold: float\n",
    "          offset_threshold: float\n",
    "          frame_threshold: float\n",
    "          pedal_offset_threshold: float\n",
    "        \"\"\"\n",
    "        self.frames_per_second = frames_per_second\n",
    "        self.classes_num = classes_num\n",
    "        self.onset_threshold = onset_threshold\n",
    "        self.offset_threshold = offset_threshold\n",
    "        self.frame_threshold = frame_threshold\n",
    "        self.pedal_offset_threshold = pedal_offset_threshold\n",
    "        self.begin_note = config.begin_note\n",
    "        self.velocity_scale = config.velocity_scale\n",
    "\n",
    "    def output_dict_to_midi_events(self, output_dict):\n",
    "        \"\"\"Main function. Post process model outputs to MIDI events.\n",
    "        Args:\n",
    "          output_dict: {\n",
    "            'reg_onset_output': (segment_frames, classes_num), \n",
    "            'reg_offset_output': (segment_frames, classes_num), \n",
    "            'frame_output': (segment_frames, classes_num), \n",
    "            'velocity_output': (segment_frames, classes_num), \n",
    "            'reg_pedal_onset_output': (segment_frames, 1), \n",
    "            'reg_pedal_offset_output': (segment_frames, 1), \n",
    "            'pedal_frame_output': (segment_frames, 1)}\n",
    "        Outputs:\n",
    "          est_note_events: list of dict, e.g. [\n",
    "            {'onset_time': 39.74, 'offset_time': 39.87, 'midi_note': 27, 'velocity': 83}, \n",
    "            {'onset_time': 11.98, 'offset_time': 12.11, 'midi_note': 33, 'velocity': 88}]\n",
    "          est_pedal_events: list of dict, e.g. [\n",
    "            {'onset_time': 0.17, 'offset_time': 0.96}, \n",
    "            {'osnet_time': 1.17, 'offset_time': 2.65}]\n",
    "        \"\"\"\n",
    "\n",
    "        # Post process piano note outputs to piano note and pedal events information\n",
    "        (est_on_off_note_vels, est_pedal_on_offs) = \\\n",
    "            self.output_dict_to_note_pedal_arrays(output_dict)\n",
    "        \"\"\"est_on_off_note_vels: (events_num, 4), the four columns are: [onset_time, offset_time, piano_note, velocity], \n",
    "        est_pedal_on_offs: (pedal_events_num, 2), the two columns are: [onset_time, offset_time]\"\"\"\n",
    "\n",
    "        # Reformat notes to MIDI events\n",
    "        est_note_events = self.detected_notes_to_events(est_on_off_note_vels)\n",
    "\n",
    "        if est_pedal_on_offs is None:\n",
    "            est_pedal_events = None\n",
    "        else:\n",
    "            est_pedal_events = self.detected_pedals_to_events(est_pedal_on_offs)\n",
    "\n",
    "        return est_note_events, est_pedal_events\n",
    "\n",
    "    def output_dict_to_note_pedal_arrays(self, output_dict):\n",
    "        \"\"\"Postprocess the output probabilities of a transription model to MIDI \n",
    "        events.\n",
    "        Args:\n",
    "          output_dict: dict, {\n",
    "            'reg_onset_output': (frames_num, classes_num), \n",
    "            'reg_offset_output': (frames_num, classes_num), \n",
    "            'frame_output': (frames_num, classes_num), \n",
    "            'velocity_output': (frames_num, classes_num), \n",
    "            ...}\n",
    "        Returns:\n",
    "          est_on_off_note_vels: (events_num, 4), the 4 columns are onset_time, \n",
    "            offset_time, piano_note and velocity. E.g. [\n",
    "             [39.74, 39.87, 27, 0.65], \n",
    "             [11.98, 12.11, 33, 0.69], \n",
    "             ...]\n",
    "          est_pedal_on_offs: (pedal_events_num, 2), the 2 columns are onset_time \n",
    "            and offset_time. E.g. [\n",
    "             [0.17, 0.96], \n",
    "             [1.17, 2.65], \n",
    "             ...]\n",
    "        \"\"\"\n",
    "\n",
    "        # ------ 1. Process regression outputs to binarized outputs ------\n",
    "        # For example, onset or offset of [0., 0., 0.15, 0.30, 0.40, 0.35, 0.20, 0.05, 0., 0.]\n",
    "        # will be processed to [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]\n",
    "\n",
    "        # Calculate binarized onset output from regression output\n",
    "        (onset_output, onset_shift_output) = \\\n",
    "            self.get_binarized_output_from_regression(\n",
    "                reg_output=output_dict['reg_onset_output'], \n",
    "                threshold=self.onset_threshold, neighbour=2)\n",
    "\n",
    "        output_dict['onset_output'] = onset_output  # Values are 0 or 1\n",
    "        output_dict['onset_shift_output'] = onset_shift_output  \n",
    "\n",
    "        # Calculate binarized offset output from regression output\n",
    "        (offset_output, offset_shift_output) = \\\n",
    "            self.get_binarized_output_from_regression(\n",
    "                reg_output=output_dict['reg_offset_output'], \n",
    "                threshold=self.offset_threshold, neighbour=4)\n",
    "\n",
    "        output_dict['offset_output'] = offset_output  # Values are 0 or 1\n",
    "        output_dict['offset_shift_output'] = offset_shift_output\n",
    "\n",
    "        if 'reg_pedal_onset_output' in output_dict.keys():\n",
    "            \"\"\"Pedal onsets are not used in inference. Instead, frame-wise pedal\n",
    "            predictions are used to detect onsets. We empirically found this is \n",
    "            more accurate to detect pedal onsets.\"\"\"\n",
    "            pass\n",
    "\n",
    "        if 'reg_pedal_offset_output' in output_dict.keys():\n",
    "            # Calculate binarized pedal offset output from regression output\n",
    "            (pedal_offset_output, pedal_offset_shift_output) = \\\n",
    "                self.get_binarized_output_from_regression(\n",
    "                    reg_output=output_dict['reg_pedal_offset_output'], \n",
    "                    threshold=self.pedal_offset_threshold, neighbour=4)\n",
    "\n",
    "            output_dict['pedal_offset_output'] = pedal_offset_output  # Values are 0 or 1\n",
    "            output_dict['pedal_offset_shift_output'] = pedal_offset_shift_output\n",
    "\n",
    "        # ------ 2. Process matrices results to event results ------\n",
    "        # Detect piano notes from output_dict\n",
    "        est_on_off_note_vels = self.output_dict_to_detected_notes(output_dict)\n",
    "\n",
    "        if 'reg_pedal_onset_output' in output_dict.keys():\n",
    "            # Detect piano pedals from output_dict\n",
    "            est_pedal_on_offs = self.output_dict_to_detected_pedals(output_dict)\n",
    " \n",
    "        else:\n",
    "            est_pedal_on_offs = None    \n",
    "\n",
    "        return est_on_off_note_vels, est_pedal_on_offs\n",
    "\n",
    "    def get_binarized_output_from_regression(self, reg_output, threshold, neighbour):\n",
    "        \"\"\"Calculate binarized output and shifts of onsets or offsets from the\n",
    "        regression results.\n",
    "        Args:\n",
    "          reg_output: (frames_num, classes_num)\n",
    "          threshold: float\n",
    "          neighbour: int\n",
    "        Returns:\n",
    "          binary_output: (frames_num, classes_num)\n",
    "          shift_output: (frames_num, classes_num)\n",
    "        \"\"\"\n",
    "        binary_output = np.zeros_like(reg_output)\n",
    "        shift_output = np.zeros_like(reg_output)\n",
    "        (frames_num, classes_num) = reg_output.shape\n",
    "        \n",
    "        for k in range(classes_num):\n",
    "            x = reg_output[:, k]\n",
    "            for n in range(neighbour, frames_num - neighbour):\n",
    "                if x[n] > threshold and self.is_monotonic_neighbour(x, n, neighbour):\n",
    "                    binary_output[n, k] = 1\n",
    "\n",
    "                    \"\"\"See Section III-D in [1] for deduction.\n",
    "                    [1] Q. Kong, et al., High-resolution Piano Transcription \n",
    "                    with Pedals by Regressing Onsets and Offsets Times, 2020.\"\"\"\n",
    "                    if x[n - 1] > x[n + 1]:\n",
    "                        shift = (x[n + 1] - x[n - 1]) / (x[n] - x[n + 1]) / 2\n",
    "                    else:\n",
    "                        shift = (x[n + 1] - x[n - 1]) / (x[n] - x[n - 1]) / 2\n",
    "                    shift_output[n, k] = shift\n",
    "\n",
    "        return binary_output, shift_output\n",
    "\n",
    "    def is_monotonic_neighbour(self, x, n, neighbour):\n",
    "        \"\"\"Detect if values are monotonic in both side of x[n].\n",
    "        Args:\n",
    "          x: (frames_num,)\n",
    "          n: int\n",
    "          neighbour: int\n",
    "        Returns:\n",
    "          monotonic: bool\n",
    "        \"\"\"\n",
    "        monotonic = True\n",
    "        for i in range(neighbour):\n",
    "            if x[n - i] < x[n - i - 1]:\n",
    "                monotonic = False\n",
    "            if x[n + i] < x[n + i + 1]:\n",
    "                monotonic = False\n",
    "\n",
    "        return monotonic\n",
    "\n",
    "    def output_dict_to_detected_notes(self, output_dict):\n",
    "        \"\"\"Postprocess output_dict to piano notes.\n",
    "        Args:\n",
    "          output_dict: dict, e.g. {\n",
    "            'onset_output': (frames_num, classes_num),\n",
    "            'onset_shift_output': (frames_num, classes_num),\n",
    "            'offset_output': (frames_num, classes_num),\n",
    "            'offset_shift_output': (frames_num, classes_num),\n",
    "            'frame_output': (frames_num, classes_num),\n",
    "            'onset_output': (frames_num, classes_num),\n",
    "            ...}\n",
    "        Returns:\n",
    "          est_on_off_note_vels: (notes, 4), the four columns are onsets, offsets, \n",
    "          MIDI notes and velocities. E.g.,\n",
    "            [[39.7375, 39.7500, 27., 0.6638],\n",
    "             [11.9824, 12.5000, 33., 0.6892],\n",
    "             ...]\n",
    "        \"\"\"\n",
    "        est_tuples = []\n",
    "        est_midi_notes = []\n",
    "        classes_num = output_dict['frame_output'].shape[-1]\n",
    " \n",
    "        for piano_note in range(classes_num):\n",
    "            \"\"\"Detect piano notes\"\"\"\n",
    "            est_tuples_per_note = note_detection_with_onset_offset_regress(\n",
    "                frame_output=output_dict['frame_output'][:, piano_note], \n",
    "                onset_output=output_dict['onset_output'][:, piano_note], \n",
    "                onset_shift_output=output_dict['onset_shift_output'][:, piano_note], \n",
    "                offset_output=output_dict['offset_output'][:, piano_note], \n",
    "                offset_shift_output=output_dict['offset_shift_output'][:, piano_note], \n",
    "                velocity_output=output_dict['velocity_output'][:, piano_note], \n",
    "                frame_threshold=self.frame_threshold)\n",
    "            \n",
    "            est_tuples += est_tuples_per_note\n",
    "            est_midi_notes += [piano_note + self.begin_note] * len(est_tuples_per_note)\n",
    "\n",
    "        est_tuples = np.array(est_tuples)   # (notes, 5)\n",
    "        \"\"\"(notes, 5), the five columns are onset, offset, onset_shift, \n",
    "        offset_shift and normalized_velocity\"\"\"\n",
    "\n",
    "        est_midi_notes = np.array(est_midi_notes) # (notes,)\n",
    "\n",
    "        onset_times = (est_tuples[:, 0] + est_tuples[:, 2]) / self.frames_per_second\n",
    "        offset_times = (est_tuples[:, 1] + est_tuples[:, 3]) / self.frames_per_second\n",
    "        velocities = est_tuples[:, 4]\n",
    "        \n",
    "        est_on_off_note_vels = np.stack((onset_times, offset_times, est_midi_notes, velocities), axis=-1)\n",
    "        \"\"\"(notes, 3), the three columns are onset_times, offset_times and velocity.\"\"\"\n",
    "\n",
    "        est_on_off_note_vels = est_on_off_note_vels.astype(np.float32)\n",
    "\n",
    "        return est_on_off_note_vels\n",
    "\n",
    "    def output_dict_to_detected_pedals(self, output_dict):\n",
    "        \"\"\"Postprocess output_dict to piano pedals.\n",
    "        Args:\n",
    "          output_dict: dict, e.g. {\n",
    "            'pedal_frame_output': (frames_num,),\n",
    "            'pedal_offset_output': (frames_num,),\n",
    "            'pedal_offset_shift_output': (frames_num,),\n",
    "            ...}\n",
    "        Returns:\n",
    "          est_on_off: (notes, 2), the two columns are pedal onsets and pedal\n",
    "            offsets. E.g.,\n",
    "              [[0.1800, 0.9669],\n",
    "               [1.1400, 2.6458],\n",
    "               ...]\n",
    "        \"\"\"\n",
    "        frames_num = output_dict['pedal_frame_output'].shape[0]\n",
    "        \n",
    "        est_tuples = pedal_detection_with_onset_offset_regress(\n",
    "            frame_output=output_dict['pedal_frame_output'][:, 0], \n",
    "            offset_output=output_dict['pedal_offset_output'][:, 0], \n",
    "            offset_shift_output=output_dict['pedal_offset_shift_output'][:, 0], \n",
    "            frame_threshold=0.5)\n",
    "\n",
    "        est_tuples = np.array(est_tuples)\n",
    "        \"\"\"(notes, 2), the two columns are pedal onsets and pedal offsets\"\"\"\n",
    "        \n",
    "        if len(est_tuples) == 0:\n",
    "            return np.array([])\n",
    "\n",
    "        else:\n",
    "            onset_times = (est_tuples[:, 0] + est_tuples[:, 2]) / self.frames_per_second\n",
    "            offset_times = (est_tuples[:, 1] + est_tuples[:, 3]) / self.frames_per_second\n",
    "            est_on_off = np.stack((onset_times, offset_times), axis=-1)\n",
    "            est_on_off = est_on_off.astype(np.float32)\n",
    "            return est_on_off\n",
    "\n",
    "    def detected_notes_to_events(self, est_on_off_note_vels):\n",
    "        \"\"\"Reformat detected notes to midi events.\n",
    "        Args:\n",
    "          est_on_off_vels: (notes, 3), the three columns are onset_times, \n",
    "            offset_times and velocity. E.g.\n",
    "            [[32.8376, 35.7700, 0.7932],\n",
    "             [37.3712, 39.9300, 0.8058],\n",
    "             ...]\n",
    "        \n",
    "        Returns:\n",
    "          midi_events, list, e.g.,\n",
    "            [{'onset_time': 39.7376, 'offset_time': 39.75, 'midi_note': 27, 'velocity': 84},\n",
    "             {'onset_time': 11.9824, 'offset_time': 12.50, 'midi_note': 33, 'velocity': 88},\n",
    "             ...]\n",
    "        \"\"\"\n",
    "        midi_events = []\n",
    "        for i in range(est_on_off_note_vels.shape[0]):\n",
    "            midi_events.append({\n",
    "                'onset_time': est_on_off_note_vels[i][0], \n",
    "                'offset_time': est_on_off_note_vels[i][1], \n",
    "                'midi_note': int(est_on_off_note_vels[i][2]), \n",
    "                'velocity': int(est_on_off_note_vels[i][3] * self.velocity_scale)})\n",
    "\n",
    "        return midi_events\n",
    "\n",
    "    def detected_pedals_to_events(self, pedal_on_offs):\n",
    "        \"\"\"Reformat detected pedal onset and offsets to events.\n",
    "        Args:\n",
    "          pedal_on_offs: (notes, 2), the two columns are pedal onsets and pedal\n",
    "          offsets. E.g., \n",
    "            [[0.1800, 0.9669],\n",
    "             [1.1400, 2.6458],\n",
    "             ...]\n",
    "        Returns:\n",
    "          pedal_events: list of dict, e.g.,\n",
    "            [{'onset_time': 0.1800, 'offset_time': 0.9669}, \n",
    "             {'onset_time': 1.1400, 'offset_time': 2.6458},\n",
    "             ...]\n",
    "        \"\"\"\n",
    "        pedal_events = []\n",
    "        for i in range(len(pedal_on_offs)):\n",
    "            pedal_events.append({\n",
    "                'onset_time': pedal_on_offs[i, 0], \n",
    "                'offset_time': pedal_on_offs[i, 1]})\n",
    "        \n",
    "        return pedal_events\n",
    "\n",
    "\n",
    "class OnsetsFramesPostProcessor(object):\n",
    "    def __init__(self, frames_per_second, classes_num):\n",
    "        \"\"\"Postprocess the Googl's onsets and frames system output. Only used\n",
    "        for comparison.\n",
    "        Args:\n",
    "          frames_per_second: int\n",
    "          classes_num: int\n",
    "        \"\"\"\n",
    "        self.frames_per_second = frames_per_second\n",
    "        self.classes_num = classes_num\n",
    "        self.begin_note = config.begin_note\n",
    "        self.velocity_scale = config.velocity_scale\n",
    "        \n",
    "        self.frame_threshold = 0.5\n",
    "        self.onset_threshold = 0.1\n",
    "        self.offset_threshold = 0.3\n",
    "\n",
    "    def output_dict_to_midi_events(self, output_dict):\n",
    "        \"\"\"Main function. Post process model outputs to MIDI events.\n",
    "        Args:\n",
    "          output_dict: {\n",
    "            'reg_onset_output': (segment_frames, classes_num), \n",
    "            'reg_offset_output': (segment_frames, classes_num), \n",
    "            'frame_output': (segment_frames, classes_num), \n",
    "            'velocity_output': (segment_frames, classes_num), \n",
    "            'reg_pedal_onset_output': (segment_frames, 1), \n",
    "            'reg_pedal_offset_output': (segment_frames, 1), \n",
    "            'pedal_frame_output': (segment_frames, 1)}\n",
    "        Outputs:\n",
    "          est_note_events: list of dict, e.g. [\n",
    "            {'onset_time': 39.74, 'offset_time': 39.87, 'midi_note': 27, 'velocity': 83}, \n",
    "            {'onset_time': 11.98, 'offset_time': 12.11, 'midi_note': 33, 'velocity': 88}]\n",
    "          est_pedal_events: list of dict, e.g. [\n",
    "            {'onset_time': 0.17, 'offset_time': 0.96}, \n",
    "            {'osnet_time': 1.17, 'offset_time': 2.65}]\n",
    "        \"\"\"\n",
    "\n",
    "        # Post process piano note outputs to piano note and pedal events information\n",
    "        (est_on_off_note_vels, est_pedal_on_offs) = \\\n",
    "            self.output_dict_to_note_pedal_arrays(output_dict)\n",
    "        \"\"\"est_on_off_note_vels: (events_num, 4), the four columns are: [onset_time, offset_time, piano_note, velocity], \n",
    "        est_pedal_on_offs: (pedal_events_num, 2), the two columns are: [onset_time, offset_time]\"\"\"\n",
    "        \n",
    "        # Reformat notes to MIDI events\n",
    "        est_note_events = self.detected_notes_to_events(est_on_off_note_vels)\n",
    "\n",
    "        if est_pedal_on_offs is None:\n",
    "            est_pedal_events = None\n",
    "        else:\n",
    "            est_pedal_events = self.detected_pedals_to_events(est_pedal_on_offs)\n",
    "\n",
    "        return est_note_events, est_pedal_events\n",
    "\n",
    "    def output_dict_to_note_pedal_arrays(self, output_dict):\n",
    "        \"\"\"Postprocess the output probabilities of a transription model to MIDI \n",
    "        events.\n",
    "        Args:\n",
    "          output_dict: dict, {\n",
    "            'reg_onset_output': (frames_num, classes_num), \n",
    "            'reg_offset_output': (frames_num, classes_num), \n",
    "            'frame_output': (frames_num, classes_num), \n",
    "            'velocity_output': (frames_num, classes_num), \n",
    "            ...}\n",
    "        Returns:\n",
    "          est_on_off_note_vels: (events_num, 4), the 4 columns are onset_time, \n",
    "            offset_time, piano_note and velocity. E.g. [\n",
    "             [39.74, 39.87, 27, 0.65], \n",
    "             [11.98, 12.11, 33, 0.69], \n",
    "             ...]\n",
    "          est_pedal_on_offs: (pedal_events_num, 2), the 2 columns are onset_time \n",
    "            and offset_time. E.g. [\n",
    "             [0.17, 0.96], \n",
    "             [1.17, 2.65], \n",
    "             ...]\n",
    "        \"\"\"\n",
    "\n",
    "        # Sharp onsets and offsets\n",
    "        output_dict = self.sharp_output_dict(\n",
    "            output_dict, onset_threshold=self.onset_threshold, \n",
    "            offset_threshold=self.offset_threshold)\n",
    "\n",
    "        # Post process output_dict to piano notes\n",
    "        est_on_off_note_vels = self.output_dict_to_detected_notes(output_dict, \n",
    "            frame_threshold=self.frame_threshold)\n",
    "\n",
    "        if 'reg_pedal_onset_output' in output_dict.keys():\n",
    "            # Detect piano pedals from output_dict\n",
    "            est_pedal_on_offs = self.output_dict_to_detected_pedals(output_dict)\n",
    " \n",
    "        else:\n",
    "            est_pedal_on_offs = None    \n",
    "\n",
    "        return est_on_off_note_vels, est_pedal_on_offs\n",
    "\n",
    "    def sharp_output_dict(self, output_dict, onset_threshold, offset_threshold):\n",
    "        \"\"\"Sharp onsets and offsets. E.g. when threshold=0.3, for a note, \n",
    "        [0, 0.1, 0.4, 0.7, 0, 0] will be sharped to [0, 0, 0, 1, 0, 0]\n",
    "        [0., 0., 1., 0., 0., 0.]\n",
    "        Args:\n",
    "          output_dict: {\n",
    "            'reg_onset_output': (frames_num, classes_num), \n",
    "            'reg_offset_output': (frames_num, classes_num), \n",
    "            ...}\n",
    "          onset_threshold: float\n",
    "          offset_threshold: float\n",
    "        Returns:\n",
    "          output_dict: {\n",
    "            'onset_output': (frames_num, classes_num), \n",
    "            'offset_output': (frames_num, classes_num)}\n",
    "        \"\"\"\n",
    "        if 'reg_onset_output' in output_dict.keys():\n",
    "            output_dict['onset_output'] = self.sharp_output(\n",
    "                output_dict['reg_onset_output'], \n",
    "                threshold=onset_threshold)\n",
    "\n",
    "        if 'reg_offset_output' in output_dict.keys():\n",
    "            output_dict['offset_output'] = self.sharp_output(\n",
    "                output_dict['reg_offset_output'], \n",
    "                threshold=offset_threshold)\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def sharp_output(self, input, threshold=0.3):\n",
    "        \"\"\"Used for sharping onset or offset. E.g. when threshold=0.3, for a note, \n",
    "        [0, 0.1, 0.4, 0.7, 0, 0] will be sharped to [0, 0, 0, 1, 0, 0]\n",
    "        Args:\n",
    "          input: (frames_num, classes_num)\n",
    "        Returns:\n",
    "          output: (frames_num, classes_num)\n",
    "        \"\"\"\n",
    "        (frames_num, classes_num) = input.shape\n",
    "        output = np.zeros_like(input)\n",
    "\n",
    "        for piano_note in range(classes_num):\n",
    "            loct = None\n",
    "            for i in range(1, frames_num - 1):\n",
    "                if input[i, piano_note] > threshold and input[i, piano_note] > input[i - 1, piano_note] and input[i, piano_note] > input[i + 1, piano_note]:\n",
    "                    loct = i\n",
    "                else:\n",
    "                    if loct is not None:\n",
    "                        output[loct, piano_note] = 1\n",
    "                        loct = None\n",
    "\n",
    "        return output\n",
    "\n",
    "    def output_dict_to_detected_notes(self, output_dict, frame_threshold):\n",
    "        \"\"\"Postprocess output_dict to piano notes.\n",
    "        Args:\n",
    "          output_dict: dict, e.g. {\n",
    "            'onset_output': (frames_num, classes_num),\n",
    "            'onset_shift_output': (frames_num, classes_num),\n",
    "            'offset_output': (frames_num, classes_num),\n",
    "            'offset_shift_output': (frames_num, classes_num),\n",
    "            'frame_output': (frames_num, classes_num),\n",
    "            'onset_output': (frames_num, classes_num),\n",
    "            ...}\n",
    "        Returns:\n",
    "          est_on_off_note_vels: (notes, 4), the four columns are onsets, offsets, \n",
    "          MIDI notes and velocities. E.g.,\n",
    "            [[39.7375, 39.7500, 27., 0.6638],\n",
    "             [11.9824, 12.5000, 33., 0.6892],\n",
    "             ...]\n",
    "        \"\"\"\n",
    "\n",
    "        est_tuples = []\n",
    "        est_midi_notes = []\n",
    "\n",
    "        for piano_note in range(self.classes_num):\n",
    "            \n",
    "            est_tuples_per_note = onsets_frames_note_detection(\n",
    "                frame_output=output_dict['frame_output'][:, piano_note], \n",
    "                onset_output=output_dict['onset_output'][:, piano_note], \n",
    "                offset_output=output_dict['offset_output'][:, piano_note], \n",
    "                velocity_output=output_dict['velocity_output'][:, piano_note], \n",
    "                threshold=frame_threshold)\n",
    "\n",
    "            est_tuples += est_tuples_per_note\n",
    "            est_midi_notes += [piano_note + self.begin_note] * len(est_tuples_per_note)\n",
    "\n",
    "        est_tuples = np.array(est_tuples)   # (notes, 3)\n",
    "        \"\"\"(notes, 5), the five columns are onset, offset, onset_shift, \n",
    "        offset_shift and normalized_velocity\"\"\"\n",
    "\n",
    "        est_midi_notes = np.array(est_midi_notes) # (notes,)\n",
    "        \n",
    "        if len(est_midi_notes) == 0:\n",
    "            return []\n",
    "        else:\n",
    "            onset_times = est_tuples[:, 0] / self.frames_per_second\n",
    "            offset_times = est_tuples[:, 1] / self.frames_per_second\n",
    "            velocities = est_tuples[:, 2]\n",
    "        \n",
    "            est_on_off_note_vels = np.stack((onset_times, offset_times, est_midi_notes, velocities), axis=-1)\n",
    "            \"\"\"(notes, 3), the three columns are onset_times, offset_times and velocity.\"\"\"\n",
    "\n",
    "            est_on_off_note_vels = est_on_off_note_vels.astype(np.float32)\n",
    "\n",
    "            return est_on_off_note_vels\n",
    "\n",
    "    def output_dict_to_detected_pedals(self, output_dict):\n",
    "        \"\"\"Postprocess output_dict to piano pedals.\n",
    "        Args:\n",
    "          output_dict: dict, e.g. {\n",
    "            'pedal_frame_output': (frames_num,),\n",
    "            'pedal_offset_output': (frames_num,),\n",
    "            'pedal_offset_shift_output': (frames_num,),\n",
    "            ...}\n",
    "        Returns:\n",
    "          est_on_off: (notes, 2), the two columns are pedal onsets and pedal\n",
    "            offsets. E.g.,\n",
    "              [[0.1800, 0.9669],\n",
    "               [1.1400, 2.6458],\n",
    "               ...]\n",
    "        \"\"\"\n",
    "\n",
    "        frames_num = output_dict['pedal_frame_output'].shape[0]\n",
    "        \n",
    "        est_tuples = onsets_frames_pedal_detection(\n",
    "            frame_output=output_dict['pedal_frame_output'][:, 0], \n",
    "            offset_output=output_dict['reg_pedal_offset_output'][:, 0], \n",
    "            frame_threshold=0.5)\n",
    "\n",
    "        est_tuples = np.array(est_tuples)\n",
    "        \"\"\"(notes, 2), the two columns are pedal onsets and pedal offsets\"\"\"\n",
    "        \n",
    "        if len(est_tuples) == 0:\n",
    "            return np.array([])\n",
    "\n",
    "        else:\n",
    "            onset_times = est_tuples[:, 0] / self.frames_per_second\n",
    "            offset_times = est_tuples[:, 1] / self.frames_per_second\n",
    "            est_on_off = np.stack((onset_times, offset_times), axis=-1)\n",
    "            est_on_off = est_on_off.astype(np.float32)\n",
    "            return est_on_off\n",
    "\n",
    "    def detected_notes_to_events(self, est_on_off_note_vels):\n",
    "        \"\"\"Reformat detected notes to midi events.\n",
    "        Args:\n",
    "          est_on_off_vels: (notes, 3), the three columns are onset_times, \n",
    "            offset_times and velocity. E.g.\n",
    "            [[32.8376, 35.7700, 0.7932],\n",
    "             [37.3712, 39.9300, 0.8058],\n",
    "             ...]\n",
    "        \n",
    "        Returns:\n",
    "          midi_events, list, e.g.,\n",
    "            [{'onset_time': 39.7376, 'offset_time': 39.75, 'midi_note': 27, 'velocity': 84},\n",
    "             {'onset_time': 11.9824, 'offset_time': 12.50, 'midi_note': 33, 'velocity': 88},\n",
    "             ...]\n",
    "        \"\"\"\n",
    "        midi_events = []\n",
    "        for i in range(len(est_on_off_note_vels)):\n",
    "            midi_events.append({\n",
    "                'onset_time': est_on_off_note_vels[i][0], \n",
    "                'offset_time': est_on_off_note_vels[i][1], \n",
    "                'midi_note': int(est_on_off_note_vels[i][2]), \n",
    "                'velocity': int(est_on_off_note_vels[i][3] * self.velocity_scale)})\n",
    "\n",
    "        return midi_events\n",
    "\n",
    "    def detected_pedals_to_events(self, pedal_on_offs):\n",
    "        \"\"\"Reformat detected pedal onset and offsets to events.\n",
    "        Args:\n",
    "          pedal_on_offs: (notes, 2), the two columns are pedal onsets and pedal\n",
    "          offsets. E.g., \n",
    "            [[0.1800, 0.9669],\n",
    "             [1.1400, 2.6458],\n",
    "             ...]\n",
    "        Returns:\n",
    "          pedal_events: list of dict, e.g.,\n",
    "            [{'onset_time': 0.1800, 'offset_time': 0.9669}, \n",
    "             {'onset_time': 1.1400, 'offset_time': 2.6458},\n",
    "             ...]\n",
    "        \"\"\"\n",
    "        pedal_events = []\n",
    "        for i in range(len(pedal_on_offs)):\n",
    "            pedal_events.append({\n",
    "                'onset_time': pedal_on_offs[i, 0], \n",
    "                'offset_time': pedal_on_offs[i, 1]})\n",
    "        \n",
    "        return pedal_events\n",
    "\n",
    "\n",
    "class StatisticsContainer(object):\n",
    "    def __init__(self, statistics_path):\n",
    "        \"\"\"Contain statistics of different training iterations.\n",
    "        \"\"\"\n",
    "        self.statistics_path = statistics_path\n",
    "\n",
    "        self.backup_statistics_path = '{}_{}.pkl'.format(\n",
    "            os.path.splitext(self.statistics_path)[0], \n",
    "            datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "\n",
    "        self.statistics_dict = {'train': [], 'validation': [], 'test': []}\n",
    "\n",
    "    def append(self, iteration, statistics, data_type):\n",
    "        statistics['iteration'] = iteration\n",
    "        self.statistics_dict[data_type].append(statistics)\n",
    "        \n",
    "    def dump(self):\n",
    "        pickle.dump(self.statistics_dict, open(self.statistics_path, 'wb'))\n",
    "        pickle.dump(self.statistics_dict, open(self.backup_statistics_path, 'wb'))\n",
    "        logging.info('    Dump statistics to {}'.format(self.statistics_path))\n",
    "        logging.info('    Dump statistics to {}'.format(self.backup_statistics_path))\n",
    "        \n",
    "    def load_state_dict(self, resume_iteration):\n",
    "        self.statistics_dict = pickle.load(open(self.statistics_path, 'rb'))\n",
    "\n",
    "        resume_statistics_dict = {'train': [], 'validation': [], 'test': []}\n",
    "        \n",
    "        for key in self.statistics_dict.keys():\n",
    "            for statistics in self.statistics_dict[key]:\n",
    "                if statistics['iteration'] <= resume_iteration:\n",
    "                    resume_statistics_dict[key].append(statistics)\n",
    "                \n",
    "        self.statistics_dict = resume_statistics_dict\n",
    "\n",
    "\n",
    "def load_audio(path, sr=22050, mono=True, offset=0.0, duration=None,\n",
    "    dtype=np.float32, res_type='kaiser_best', \n",
    "    backends=[audioread.ffdec.FFmpegAudioFile]):\n",
    "    \"\"\"Load audio. Copied from librosa.core.load() except that ffmpeg backend is \n",
    "    always used in this function.\"\"\"\n",
    "\n",
    "    y = []\n",
    "    with audioread.audio_open(os.path.realpath(path), backends=backends) as input_file:\n",
    "        sr_native = input_file.samplerate\n",
    "        n_channels = input_file.channels\n",
    "\n",
    "        s_start = int(np.round(sr_native * offset)) * n_channels\n",
    "\n",
    "        if duration is None:\n",
    "            s_end = np.inf\n",
    "        else:\n",
    "            s_end = s_start + (int(np.round(sr_native * duration))\n",
    "                               * n_channels)\n",
    "\n",
    "        n = 0\n",
    "\n",
    "        for frame in input_file:\n",
    "            frame = librosa.core.audio.util.buf_to_float(frame, dtype=dtype)\n",
    "            n_prev = n\n",
    "            n = n + len(frame)\n",
    "\n",
    "            if n < s_start:\n",
    "                # offset is after the current frame\n",
    "                # keep reading\n",
    "                continue\n",
    "\n",
    "            if s_end < n_prev:\n",
    "                # we're off the end.  stop reading\n",
    "                break\n",
    "\n",
    "            if s_end < n:\n",
    "                # the end is in this frame.  crop.\n",
    "                frame = frame[:s_end - n_prev]\n",
    "\n",
    "            if n_prev <= s_start <= n:\n",
    "                # beginning is in this frame\n",
    "                frame = frame[(s_start - n_prev):]\n",
    "\n",
    "            # tack on the current frame\n",
    "            y.append(frame)\n",
    "\n",
    "    if y:\n",
    "        y = np.concatenate(y)\n",
    "\n",
    "        if n_channels > 1:\n",
    "            y = y.reshape((-1, n_channels)).T\n",
    "            if mono:\n",
    "                y = librosa.core.audio.to_mono(y)\n",
    "\n",
    "        if sr is not None:\n",
    "            y = librosa.core.audio.resample(y, sr_native, sr, res_type=res_type)\n",
    "\n",
    "        else:\n",
    "            sr = sr_native\n",
    "\n",
    "    # Final cleanup for dtype and contiguity\n",
    "    y = np.ascontiguousarray(y, dtype=dtype)\n",
    "\n",
    "    return (y, sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
