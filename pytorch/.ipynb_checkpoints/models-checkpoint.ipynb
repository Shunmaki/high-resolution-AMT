{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e74ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank # use 'magphase' to extract phase informatinon with magnitude\n",
    "# ref: https://github.com/qiuqiangkong/torchlibrosa\n",
    "#from pytorch_utils import move_data_to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53134761",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3e27204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and conv block\n",
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    " \n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "            \n",
    "    \n",
    "def init_bn(bn):\n",
    "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.)\n",
    "\n",
    "\n",
    "def init_gru(rnn):\n",
    "    \"\"\"Initialize a GRU layer. \"\"\"\n",
    "    \n",
    "    def _concat_init(tensor, init_funcs):\n",
    "        (length, fan_out) = tensor.shape\n",
    "        fan_in = length // len(init_funcs)\n",
    "    \n",
    "        for (i, init_func) in enumerate(init_funcs):\n",
    "            init_func(tensor[i * fan_in : (i + 1) * fan_in, :])\n",
    "        \n",
    "    def _inner_uniform(tensor):\n",
    "        fan_in = nn.init._calculate_correct_fan(tensor, 'fan_in')\n",
    "        nn.init.uniform_(tensor, -math.sqrt(3 / fan_in), math.sqrt(3 / fan_in))\n",
    "    \n",
    "    for i in range(rnn.num_layers):\n",
    "        _concat_init(\n",
    "            getattr(rnn, 'weight_ih_l{}'.format(i)),\n",
    "            [_inner_uniform, _inner_uniform, _inner_uniform]\n",
    "        )\n",
    "        torch.nn.init.constant_(getattr(rnn, 'bias_ih_l{}'.format(i)), 0)\n",
    "\n",
    "        _concat_init(\n",
    "            getattr(rnn, 'weight_hh_l{}'.format(i)),\n",
    "            [_inner_uniform, _inner_uniform, nn.init.orthogonal_]\n",
    "        )\n",
    "        torch.nn.init.constant_(getattr(rnn, 'bias_hh_l{}'.format(i)), 0)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, momentum):\n",
    "        \n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3, 3), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "                              \n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3, 3), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "                              \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels, momentum)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels, momentum)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "        \n",
    "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input: (batch_size, in_channels, time_steps, freq_bins)\n",
    "        Outputs:\n",
    "          output: (batch_size, out_channels, classes_num)\n",
    "        \"\"\"\n",
    "\n",
    "        x = F.relu_(self.bn1(self.conv1(input)))\n",
    "        #print(x)\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        if pool_type == 'avg':\n",
    "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de187db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# アコースティックモデル\n",
    "class AcousticModelCRnn8Dropout(nn.Module):\n",
    "    def __init__(self, classes_num, midfeat, momentum):\n",
    "        super(AcousticModelCRnn8Dropout, self).__init__()\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=48, momentum=momentum)\n",
    "        self.conv_block2 = ConvBlock(in_channels=48, out_channels=64, momentum=momentum)\n",
    "        self.conv_block3 = ConvBlock(in_channels=64, out_channels=96, momentum=momentum)\n",
    "        self.conv_block4 = ConvBlock(in_channels=96, out_channels=128, momentum=momentum)\n",
    "\n",
    "        self.fc5 = nn.Linear(midfeat, 768, bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(768, momentum=momentum)\n",
    "\n",
    "        self.gru = nn.GRU(input_size=768, hidden_size=256, num_layers=2, \n",
    "            bias=True, batch_first=True, dropout=0., bidirectional=True)\n",
    "\n",
    "        self.fc = nn.Linear(512, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.fc5)\n",
    "        init_bn(self.bn5)\n",
    "        init_gru(self.gru)\n",
    "        init_layer(self.fc)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input: (batch_size, channels_num, time_steps, freq_bins)\n",
    "        Outputs:\n",
    "          output: (batch_size, time_steps, classes_num)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.conv_block1(input, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "\n",
    "        x = x.transpose(1, 2).flatten(2)\n",
    "        x = F.relu(self.bn5(self.fc5(x).transpose(1, 2)).transpose(1, 2))\n",
    "        x = F.dropout(x, p=0.5, training=self.training, inplace=True)\n",
    "        \n",
    "        (x, _) = self.gru(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training, inplace=False)\n",
    "        output = torch.sigmoid(self.fc(x))\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f49a31dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "class Regress_onset_offset_frame_velocity_CRNN(nn.Module):\n",
    "    def __init__(self, frame_per_second, classes_num):\n",
    "        super(Regress_onset_offset_frame_velocity_CRNN, self).__init__()\n",
    "        \n",
    "        sample_rate = 16000\n",
    "        window_size = 2048\n",
    "        hop_size = sample_rate // frame_per_second\n",
    "        mel_bins = 229\n",
    "        fmin = 30\n",
    "        fmax = sample_rate // 2\n",
    "        \n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "        \n",
    "        midfeat = 1792\n",
    "        momentum = 0.01\n",
    "        \n",
    "        # Spectrogram\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size,\n",
    "            hop_length=hop_size, win_length=window_size, window=window,\n",
    "            center=center, pad_mode=pad_mode, freeze_parameters=True)\n",
    "        \n",
    "        # Logmel\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate,\n",
    "            n_fft=window_size, n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref,\n",
    "            amin=amin, top_db=top_db, freeze_parameters=True)\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm2d(mel_bins, momentum)\n",
    "        \n",
    "        self.frame_model = AcousticModelCRnn8Dropout(classes_num, midfeat, momentum)\n",
    "        self.reg_onset_model = AcousticModelCRnn8Dropout(classes_num, midfeat, momentum)\n",
    "        self.reg_offset_model = AcousticModelCRnn8Dropout(classes_num, midfeat, momentum)\n",
    "        self.velocity_model = AcousticModelCRnn8Dropout(classes_num, midfeat, momentum)\n",
    "        \n",
    "        # after CRNN block\n",
    "        # only onset and frame is required\n",
    "        # \"attention\": figs (high_resolution and exploring transformer's pottential) is different but same model expect velocity network\n",
    "        self.reg_onset_gru = nn.GRU(input_size=88 * 2, hidden_size=256, num_layers=1,\n",
    "            bias=True, batch_first=True, dropout=0., bidirectional=True)\n",
    "        self.reg_onset_fc = nn.Linear(512, classes_num, bias=True)\n",
    "        \n",
    "        self.frame_gru = nn.GRU(input_size=88 * 3, hidden_size=256, num_layers=1,\n",
    "            bias=True, batch_first=True, dropout=0., bidirectional=True)\n",
    "        self.frame_fc = nn.Linear(512, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_gru(self.reg_onset_gru)\n",
    "        init_gru(self.frame_gru)\n",
    "        init_layer(self.reg_onset_fc)\n",
    "        init_layer(self.frame_fc)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: (batch_size, data_length)\n",
    "        \n",
    "        Outputs:\n",
    "            output_dict: dict, {\n",
    "              'reg_onset_output': (batch_size, time_steps, classes_num),\n",
    "              'reg_offset_output': (batch_size, time_steps, classes_num),\n",
    "              'frame_output': (batch_size, time_steps, classes_num),\n",
    "              'velocity_output': (batch_size, time_steps, classes_num)\n",
    "            }\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.spectrogram_extractor(input) # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x) # (batch_size, 1, time_step, freq_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        frame_output = self.frame_model(x)\n",
    "        reg_onset_output = self.reg_onset_model(x)\n",
    "        reg_offset_output = self.reg_offset_model(x)\n",
    "        velocity_output = self.velocity_model(x)\n",
    "        \n",
    "        # Concatenete veloacity and onset output to regress final onset output\n",
    "        x = torch.cat((reg_onset_output, (reg_onset_output ** 0.5) * velocity_output.detach()), dim=2)\n",
    "        (x, _) = self.reg_onset_gru(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training, inplace=False)\n",
    "        reg_onset_output = torch.sigmoid(self.reg_onset_fc(x))\n",
    "        \"\"\"(batch_size, time_steps, classes_num)\"\"\"\n",
    "        \n",
    "        # concatenate on/offset and frame outputs to classifier final pitch output\n",
    "        x = torch.cat((frame_output, reg_onset_output.detach(), reg_offset_output.detach()), dim=2)\n",
    "        (x, _) = self.frame_gru(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training, inplace=False)\n",
    "        frame_output = torch.sigmoid(self.frame_fc(x)) # (batch_size,  time_steps, classes_num)\n",
    "        \n",
    "        output_dict = {\n",
    "            'reg_onset_output': reg_onset_output,\n",
    "            'reg_offset_output': reg_offset_output,\n",
    "            'frame_output': frame_output,\n",
    "            'velocity_output': velocity_output}\n",
    "        \n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3d91941",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regress_pedal_CRNN(nn.Module):\n",
    "    def __init__(self, frames_per_second, classes_num):\n",
    "        super(Regress_pedal_CRNN, self).__init__()\n",
    "\n",
    "        sample_rate = 16000\n",
    "        window_size = 2048\n",
    "        hop_size = sample_rate // frames_per_second\n",
    "        mel_bins = 229\n",
    "        fmin = 30\n",
    "        fmax = sample_rate // 2\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        midfeat = 1792\n",
    "        momentum = 0.01\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, \n",
    "            hop_length=hop_size, win_length=window_size, window=window, \n",
    "            center=center, pad_mode=pad_mode, freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, \n",
    "            n_fft=window_size, n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, \n",
    "            amin=amin, top_db=top_db, freeze_parameters=True)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(mel_bins, momentum)\n",
    "\n",
    "        self.reg_pedal_onset_model = AcousticModelCRnn8Dropout(1, midfeat, momentum)\n",
    "        self.reg_pedal_offset_model = AcousticModelCRnn8Dropout(1, midfeat, momentum)\n",
    "        self.reg_pedal_frame_model = AcousticModelCRnn8Dropout(1, midfeat, momentum)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input: (batch_size, data_length)\n",
    "        Outputs:\n",
    "          output_dict: dict, {\n",
    "            'reg_onset_output': (batch_size, time_steps, classes_num),\n",
    "            'reg_offset_output': (batch_size, time_steps, classes_num),\n",
    "            'frame_output': (batch_size, time_steps, classes_num),\n",
    "            'velocity_output': (batch_size, time_steps, classes_num)\n",
    "          }\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        reg_pedal_onset_output = self.reg_pedal_onset_model(x)  # (batch_size, time_steps, classes_num)\n",
    "        reg_pedal_offset_output = self.reg_pedal_offset_model(x)  # (batch_size, time_steps, classes_num)\n",
    "        pedal_frame_output = self.reg_pedal_frame_model(x)  # (batch_size, time_steps, classes_num)\n",
    "        \n",
    "        output_dict = {\n",
    "            'reg_pedal_onset_output': reg_pedal_onset_output, \n",
    "            'reg_pedal_offset_output': reg_pedal_offset_output,\n",
    "            'pedal_frame_output': pedal_frame_output}\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c534927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is not trained, but is combined from the trained note and pedal models.\n",
    "class Note_pedal(nn.Module):\n",
    "    def __init__(self, frames_per_second, classes_num):\n",
    "        \"\"\"The combination of note and pedal model.\n",
    "        \"\"\"\n",
    "        super(Note_pedal, self).__init__()\n",
    "\n",
    "        self.note_model = Regress_onset_offset_frame_velocity_CRNN(frames_per_second, classes_num)\n",
    "        self.pedal_model = Regress_pedal_CRNN(frames_per_second, classes_num)\n",
    "\n",
    "    def load_state_dict(self, m, strict=False):\n",
    "        self.note_model.load_state_dict(m['note_model'], strict=strict)\n",
    "        self.pedal_model.load_state_dict(m['pedal_model'], strict=strict)\n",
    "\n",
    "    def forward(self, input):\n",
    "        note_output_dict = self.note_model(input)\n",
    "        pedal_output_dict = self.pedal_model(input)\n",
    "\n",
    "        full_output_dict = {}\n",
    "        full_output_dict.update(note_output_dict)\n",
    "        full_output_dict.update(pedal_output_dict)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa8218a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Regress_onset_offset_frame_velocity_CRNN(\n",
       "  (spectrogram_extractor): Spectrogram(\n",
       "    (stft): STFT(\n",
       "      (conv_real): Conv1d(1, 1025, kernel_size=(2048,), stride=(1,), bias=False)\n",
       "      (conv_imag): Conv1d(1, 1025, kernel_size=(2048,), stride=(1,), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (logmel_extractor): LogmelFilterBank()\n",
       "  (bn0): BatchNorm2d(229, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (frame_model): AcousticModelCRnn8Dropout(\n",
       "    (conv_block1): ConvBlock(\n",
       "      (conv1): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(48, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(48, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block2): ConvBlock(\n",
       "      (conv1): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(64, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block3): ConvBlock(\n",
       "      (conv1): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block4): ConvBlock(\n",
       "      (conv1): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(128, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fc5): Linear(in_features=1792, out_features=768, bias=False)\n",
       "    (bn5): BatchNorm1d(768, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (gru): GRU(768, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (fc): Linear(in_features=512, out_features=88, bias=True)\n",
       "  )\n",
       "  (reg_onset_model): AcousticModelCRnn8Dropout(\n",
       "    (conv_block1): ConvBlock(\n",
       "      (conv1): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(48, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(48, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block2): ConvBlock(\n",
       "      (conv1): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(64, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block3): ConvBlock(\n",
       "      (conv1): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block4): ConvBlock(\n",
       "      (conv1): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(128, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fc5): Linear(in_features=1792, out_features=768, bias=False)\n",
       "    (bn5): BatchNorm1d(768, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (gru): GRU(768, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (fc): Linear(in_features=512, out_features=88, bias=True)\n",
       "  )\n",
       "  (reg_offset_model): AcousticModelCRnn8Dropout(\n",
       "    (conv_block1): ConvBlock(\n",
       "      (conv1): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(48, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(48, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block2): ConvBlock(\n",
       "      (conv1): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(64, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block3): ConvBlock(\n",
       "      (conv1): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block4): ConvBlock(\n",
       "      (conv1): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(128, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fc5): Linear(in_features=1792, out_features=768, bias=False)\n",
       "    (bn5): BatchNorm1d(768, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (gru): GRU(768, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (fc): Linear(in_features=512, out_features=88, bias=True)\n",
       "  )\n",
       "  (velocity_model): AcousticModelCRnn8Dropout(\n",
       "    (conv_block1): ConvBlock(\n",
       "      (conv1): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(48, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(48, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block2): ConvBlock(\n",
       "      (conv1): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(64, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block3): ConvBlock(\n",
       "      (conv1): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block4): ConvBlock(\n",
       "      (conv1): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(128, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fc5): Linear(in_features=1792, out_features=768, bias=False)\n",
       "    (bn5): BatchNorm1d(768, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (gru): GRU(768, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (fc): Linear(in_features=512, out_features=88, bias=True)\n",
       "  )\n",
       "  (reg_onset_gru): GRU(176, 256, batch_first=True, bidirectional=True)\n",
       "  (reg_onset_fc): Linear(in_features=512, out_features=88, bias=True)\n",
       "  (frame_gru): GRU(264, 256, batch_first=True, bidirectional=True)\n",
       "  (frame_fc): Linear(in_features=512, out_features=88, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = nn.Parameter(torch.ones([2,2000])).to(device)\n",
    "net = Regress_onset_offset_frame_velocity_CRNN(frame_per_second=10000,classes_num=88)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cc5799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4908333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2001, 88])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"reg_onset_output\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aabd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
