{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b4bdda",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-55ddba1c0822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m from utilities import (create_folder, get_filename, RegressionPostProcessor, \n\u001b[1;32m     16\u001b[0m     OnsetsFramesPostProcessor, write_events_to_midi, load_audio)\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNote_pedal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmove_data_to_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join('..', '/notebooks/maki/high-resolution-AMT/utils'))\n",
    "import numpy as np\n",
    "import argparse\n",
    "import h5py\n",
    "import math\n",
    "import time\n",
    "import librosa\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    " \n",
    "from utilities import (create_folder, get_filename, RegressionPostProcessor, \n",
    "    OnsetsFramesPostProcessor, write_events_to_midi, load_audio)\n",
    "from models import Note_pedal\n",
    "from pytorch_utils import move_data_to_device, forward\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86fcea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PianoTranscription(object):\n",
    "    def __init__(self, model_type, checkpoint_path=None, \n",
    "        segment_samples=16000*10, device=torch.device('cuda'), \n",
    "        post_processor_type='regression'):\n",
    "        \"\"\"Class for transcribing piano solo recording.\n",
    "        Args:\n",
    "          model_type: str\n",
    "          checkpoint_path: str\n",
    "          segment_samples: int\n",
    "          device: 'cuda' | 'cpu'\n",
    "        \"\"\"\n",
    "\n",
    "        if 'cuda' in str(device) and torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        self.segment_samples = segment_samples\n",
    "        self.post_processor_type = post_processor_type\n",
    "        self.frames_per_second = config.frames_per_second\n",
    "        self.classes_num = config.classes_num\n",
    "        self.onset_threshold = 0.3\n",
    "        self.offset_threshod = 0.3\n",
    "        self.frame_threshold = 0.1\n",
    "        self.pedal_offset_threshold = 0.2\n",
    "\n",
    "        # Build model\n",
    "        Model = eval(model_type)\n",
    "        self.model = Model(frames_per_second=self.frames_per_second, \n",
    "            classes_num=self.classes_num)\n",
    "\n",
    "        # Load model\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model'], strict=False)\n",
    "\n",
    "        # Parallel\n",
    "        if 'cuda' in str(self.device):\n",
    "            self.model.to(self.device)\n",
    "            print('GPU number: {}'.format(torch.cuda.device_count()))\n",
    "            self.model = torch.nn.DataParallel(self.model)\n",
    "        else:\n",
    "            print('Using CPU.')\n",
    "\n",
    "    def transcribe(self, audio, midi_path):\n",
    "        \"\"\"Transcribe an audio recording.\n",
    "        Args:\n",
    "          audio: (audio_samples,)\n",
    "          midi_path: str, path to write out the transcribed MIDI.\n",
    "        Returns:\n",
    "          transcribed_dict, dict: {'output_dict':, ..., 'est_note_events': ..., \n",
    "            'est_pedal_events': ...}\n",
    "        \"\"\"\n",
    "\n",
    "        audio = audio[None, :]  # (1, audio_samples)\n",
    "\n",
    "        # Pad audio to be evenly divided by segment_samples\n",
    "        audio_len = audio.shape[1]\n",
    "        pad_len = int(np.ceil(audio_len / self.segment_samples)) \\\n",
    "            * self.segment_samples - audio_len\n",
    "\n",
    "        audio = np.concatenate((audio, np.zeros((1, pad_len))), axis=1)\n",
    "\n",
    "        # Enframe to segments\n",
    "        segments = self.enframe(audio, self.segment_samples)\n",
    "        \"\"\"(N, segment_samples)\"\"\"\n",
    "\n",
    "        # Forward\n",
    "        output_dict = forward(self.model, segments, batch_size=1)\n",
    "        \"\"\"{'reg_onset_output': (N, segment_frames, classes_num), ...}\"\"\"\n",
    "\n",
    "        # Deframe to original length\n",
    "        for key in output_dict.keys():\n",
    "            output_dict[key] = self.deframe(output_dict[key])[0 : audio_len]\n",
    "        \"\"\"output_dict: {\n",
    "          'reg_onset_output': (segment_frames, classes_num), \n",
    "          'reg_offset_output': (segment_frames, classes_num), \n",
    "          'frame_output': (segment_frames, classes_num), \n",
    "          'velocity_output': (segment_frames, classes_num), \n",
    "          'reg_pedal_onset_output': (segment_frames, 1), \n",
    "          'reg_pedal_offset_output': (segment_frames, 1), \n",
    "          'pedal_frame_output': (segment_frames, 1)}\"\"\"\n",
    "\n",
    "        # Post processor\n",
    "        if self.post_processor_type == 'regression':\n",
    "            \"\"\"Proposed high-resolution regression post processing algorithm.\"\"\"\n",
    "            post_processor = RegressionPostProcessor(self.frames_per_second, \n",
    "                classes_num=self.classes_num, onset_threshold=self.onset_threshold, \n",
    "                offset_threshold=self.offset_threshod, \n",
    "                frame_threshold=self.frame_threshold, \n",
    "                pedal_offset_threshold=self.pedal_offset_threshold)\n",
    "\n",
    "        elif self.post_processor_type == 'onsets_frames':\n",
    "            \"\"\"Google's onsets and frames post processing algorithm. Only used \n",
    "            for comparison.\"\"\"\n",
    "            post_processor = OnsetsFramesPostProcessor(self.frames_per_second, \n",
    "                self.classes_num)\n",
    "\n",
    "        # Post process output_dict to MIDI events\n",
    "        (est_note_events, est_pedal_events) = \\\n",
    "            post_processor.output_dict_to_midi_events(output_dict)\n",
    "\n",
    "        # Write MIDI events to file\n",
    "        if midi_path:\n",
    "            write_events_to_midi(start_time=0, note_events=est_note_events, \n",
    "                pedal_events=est_pedal_events, midi_path=midi_path)\n",
    "            print('Write out to {}'.format(midi_path))\n",
    "\n",
    "        transcribed_dict = {\n",
    "            'output_dict': output_dict, \n",
    "            'est_note_events': est_note_events,\n",
    "            'est_pedal_events': est_pedal_events}\n",
    "\n",
    "        return transcribed_dict\n",
    "\n",
    "    def enframe(self, x, segment_samples):\n",
    "        \"\"\"Enframe long sequence to short segments.\n",
    "        Args:\n",
    "          x: (1, audio_samples)\n",
    "          segment_samples: int\n",
    "        Returns:\n",
    "          batch: (N, segment_samples)\n",
    "        \"\"\"\n",
    "        assert x.shape[1] % segment_samples == 0\n",
    "        batch = []\n",
    "\n",
    "        pointer = 0\n",
    "        while pointer + segment_samples <= x.shape[1]:\n",
    "            batch.append(x[:, pointer : pointer + segment_samples])\n",
    "            pointer += segment_samples // 2\n",
    "\n",
    "        batch = np.concatenate(batch, axis=0)\n",
    "        return batch\n",
    "\n",
    "    def deframe(self, x):\n",
    "        \"\"\"Deframe predicted segments to original sequence.\n",
    "        Args:\n",
    "          x: (N, segment_frames, classes_num)\n",
    "        Returns:\n",
    "          y: (audio_frames, classes_num)\n",
    "        \"\"\"\n",
    "        if x.shape[0] == 1:\n",
    "            return x[0]\n",
    "\n",
    "        else:\n",
    "            x = x[:, 0 : -1, :]\n",
    "            \"\"\"Remove an extra frame in the end of each segment caused by the\n",
    "            'center=True' argument when calculating spectrogram.\"\"\"\n",
    "            (N, segment_samples, classes_num) = x.shape\n",
    "            assert segment_samples % 4 == 0\n",
    "\n",
    "            y = []\n",
    "            y.append(x[0, 0 : int(segment_samples * 0.75)])\n",
    "            for i in range(1, N - 1):\n",
    "                y.append(x[i, int(segment_samples * 0.25) : int(segment_samples * 0.75)])\n",
    "            y.append(x[-1, int(segment_samples * 0.25) :])\n",
    "            y = np.concatenate(y, axis=0)\n",
    "            return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bd6f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inference(args):\n",
    "    \"\"\"Inference template.\n",
    "    Args:\n",
    "      model_type: str\n",
    "      checkpoint_path: str\n",
    "      post_processor_type: 'regression' | 'onsets_frames'. High-resolution \n",
    "        system should use 'regression'. 'onsets_frames' is only used to compare\n",
    "        with Googl's onsets and frames system.\n",
    "      audio_path: str\n",
    "      cuda: bool\n",
    "    \"\"\"\n",
    "\n",
    "    # Arugments & parameters\n",
    "    model_type = args.model_type\n",
    "    checkpoint_path = args.checkpoint_path\n",
    "    post_processor_type = args.post_processor_type\n",
    "    device = 'cuda' if args.cuda and torch.cuda.is_available() else 'cpu'\n",
    "    audio_path = args.audio_path\n",
    "    \n",
    "    sample_rate = config.sample_rate\n",
    "    segment_samples = sample_rate * 10  \n",
    "    \"\"\"Split audio to multiple 10-second segments for inference\"\"\"\n",
    "\n",
    "    # Paths\n",
    "    midi_path = 'results/{}.mid'.format(get_filename(audio_path))\n",
    "    create_folder(os.path.dirname(midi_path))\n",
    " \n",
    "    # Load audio\n",
    "    (audio, _) = load_audio(audio_path, sr=sample_rate, mono=True)\n",
    "\n",
    "    # Transcriptor\n",
    "    transcriptor = PianoTranscription(model_type, device=device, \n",
    "        checkpoint_path=checkpoint_path, segment_samples=segment_samples, \n",
    "        post_processor_type=post_processor_type)\n",
    "\n",
    "    # Transcribe and write out to MIDI file\n",
    "    transcribe_time = time.time()\n",
    "    transcribed_dict = transcriptor.transcribe(audio, midi_path)\n",
    "    print('Transcribe time: {:.3f} s'.format(time.time() - transcribe_time))\n",
    "\n",
    "    # Visualize for debug\n",
    "    plot = False\n",
    "    if plot:\n",
    "        output_dict = transcribed_dict['output_dict']\n",
    "        fig, axs = plt.subplots(5, 1, figsize=(15, 8), sharex=True)\n",
    "        mel = librosa.feature.melspectrogram(audio, sr=16000, n_fft=2048, hop_length=160, n_mels=229, fmin=30, fmax=8000)\n",
    "        axs[0].matshow(np.log(mel), origin='lower', aspect='auto', cmap='jet')\n",
    "        axs[1].matshow(output_dict['frame_output'].T, origin='lower', aspect='auto', cmap='jet')\n",
    "        axs[2].matshow(output_dict['reg_onset_output'].T, origin='lower', aspect='auto', cmap='jet')\n",
    "        axs[3].matshow(output_dict['reg_offset_output'].T, origin='lower', aspect='auto', cmap='jet')\n",
    "        axs[4].plot(output_dict['pedal_frame_output'])\n",
    "        axs[0].set_xlim(0, len(output_dict['frame_output']))\n",
    "        axs[4].set_xlabel('Frames')\n",
    "        axs[0].set_title('Log mel spectrogram')\n",
    "        axs[1].set_title('frame_output')\n",
    "        axs[2].set_title('reg_onset_output')\n",
    "        axs[3].set_title('reg_offset_output')\n",
    "        axs[4].set_title('pedal_frame_output')\n",
    "        plt.tight_layout(0, .05, 0)\n",
    "        fig_path = '_zz.pdf'.format(get_filename(audio_path))\n",
    "        plt.savefig(fig_path)\n",
    "        print('Plot to {}'.format(fig_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5098333c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --model_type MODEL_TYPE --checkpoint_path\n",
      "                             CHECKPOINT_PATH\n",
      "                             [--post_processor_type {onsets_frames,regression}]\n",
      "                             --audio_path AUDIO_PATH [--cuda]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --model_type, --checkpoint_path, --audio_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='')\n",
    "    parser.add_argument('--model_type', type=str, required=True)\n",
    "    parser.add_argument('--checkpoint_path', type=str, required=True)\n",
    "    parser.add_argument('--post_processor_type', type=str, default='regression', choices=['onsets_frames', 'regression'])\n",
    "    parser.add_argument('--audio_path', type=str, required=True)\n",
    "    parser.add_argument('--cuda', action='store_true', default=False)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    inference(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7301f64a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
